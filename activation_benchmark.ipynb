{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b61e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data import get_data, preprocess_data\n",
    "from config import config\n",
    "\n",
    "with open(\"model/gfr_dataset.json\", \"r\") as f:\n",
    "    json_dataset = json.load(f)\n",
    "\n",
    "dataset = utils.df_from_json(json_dataset)\n",
    "\n",
    "cell_ids = set()\n",
    "for k in dataset.keys():\n",
    "    cell_ids = cell_ids.union(set(dataset[k][\"cell_id\"].to_list()))\n",
    "cell_ids = list(cell_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e45029",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cell_ids.csv\", \"w\") as f:\n",
    "    f.write(\",\".join(map(str, cell_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "717e4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Original Polynomial Activation\n",
    "# -----------------------------------------------------------------------------\n",
    "class PolynomialActivation(torch.nn.Module):\n",
    "    def __init__(self, degree, max_current, max_firing_rate, bin_size):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.max_current = torch.nn.Parameter(torch.tensor(max_current).reshape(1), requires_grad=False)\n",
    "        self.max_firing_rate = torch.nn.Parameter(torch.tensor(max_firing_rate).reshape(1), requires_grad=False)\n",
    "        self.bin_size = bin_size\n",
    "        self.p = torch.nn.Parameter(torch.tensor([d for d in range(degree+1)]), requires_grad=False)\n",
    "        self.poly_coeff = torch.nn.Parameter(torch.randn(1, self.degree + 1))\n",
    "        self.b = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = (z - self.b) / self.max_current\n",
    "        poly = torch.einsum(\"ijk,jk->ij\",\n",
    "                            x.unsqueeze(dim=2).pow(self.p.reshape(1, 1, -1)),\n",
    "                            self.poly_coeff ** 2)\n",
    "        tan = self.max_firing_rate * torch.tanh(poly)\n",
    "        return F.relu(tan).to(torch.float32)\n",
    "\n",
    "    def get_params(self):\n",
    "        return {\n",
    "            \"type\": \"polynomial\",\n",
    "            \"degree\": self.degree,\n",
    "            \"max_current\": self.max_current.item(),\n",
    "            \"max_firing_rate\": self.max_firing_rate.item(),\n",
    "            \"poly_coeff\": self.poly_coeff.detach().cpu().tolist(),\n",
    "            \"b\": self.b.detach().cpu().tolist(),\n",
    "            \"bin_size\": self.bin_size\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, params):\n",
    "        g = cls(params[\"degree\"], params[\"max_current\"], params[\"max_firing_rate\"], params[\"bin_size\"])\n",
    "        g.poly_coeff = torch.nn.Parameter(torch.tensor(params[\"poly_coeff\"]))\n",
    "        g.b = torch.nn.Parameter(torch.tensor(params[\"b\"]))\n",
    "        return g\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Benchmark Activation Modules\n",
    "# -----------------------------------------------------------------------------\n",
    "class ReLUActivation(torch.nn.Module):\n",
    "    def __init__(self, bin_size, max_current):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(1))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1))\n",
    "        self.bin_size = bin_size\n",
    "        self.max_current = max_current\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = (self.a * z - self.b) / self.max_current\n",
    "        return F.relu(x)\n",
    "\n",
    "    def get_params(self):\n",
    "        return {\"type\": \"relu\", \"a\": self.a.detach().item(), \"b\": self.b.detach().item(), \"bin_size\": self.bin_size}\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, params):\n",
    "        g = cls(params[\"bin_size\"])\n",
    "        g.a = torch.nn.Parameter(torch.tensor(params[\"a\"]))\n",
    "        g.b = torch.nn.Parameter(torch.tensor(params[\"b\"]))\n",
    "        return g\n",
    "\n",
    "class SigmoidActivation(torch.nn.Module):\n",
    "    def __init__(self, bin_size, max_firing_rate, max_current):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(1))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1))\n",
    "        self.c = torch.nn.Parameter(torch.tensor(max_firing_rate), requires_grad=False)  # scale\n",
    "        self.bin_size = bin_size\n",
    "        self.max_current = max_current\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = (self.a * z - self.b) / self.max_current\n",
    "        return self.c * torch.sigmoid(x)\n",
    "\n",
    "    def get_params(self):\n",
    "        return {\"type\": \"sigmoid\", \"a\": self.a.detach().item(), \"b\": self.b.detach().item(), \"c\": self.c.detach().item(), \"bin_size\": self.bin_size}\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, params):\n",
    "        g = cls(params[\"bin_size\"], params[\"c\"])\n",
    "        g.a = torch.nn.Parameter(torch.tensor(params[\"a\"]))\n",
    "        g.b = torch.nn.Parameter(torch.tensor(params[\"b\"]))\n",
    "        return g\n",
    "\n",
    "class SoftplusActivation(torch.nn.Module):\n",
    "    def __init__(self, bin_size, max_current):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(1))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1))\n",
    "        self.c = torch.nn.Parameter(torch.tensor(1.0), requires_grad=False)  # scale\n",
    "        self.bin_size = bin_size\n",
    "        self.max_current = max_current\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = (self.a * z - self.b) / self.max_current\n",
    "        return self.c * F.softplus(x)\n",
    "\n",
    "    def get_params(self):\n",
    "        return {\"type\": \"softplus\", \"a\": self.a.detach().item(), \"b\": self.b.detach().item(), \"c\": self.c.detach().item(), \"bin_size\": self.bin_size}\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, params):\n",
    "        g = cls(params[\"bin_size\"])\n",
    "        g.a = torch.nn.Parameter(torch.tensor(params[\"a\"]))\n",
    "        g.b = torch.nn.Parameter(torch.tensor(params[\"b\"]))\n",
    "        g.c = torch.nn.Parameter(torch.tensor(params[\"c\"]))\n",
    "        return g\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Utility: Choose activation class\n",
    "# -----------------------------------------------------------------------------\n",
    "def make_activation(act_type: str, bin_size: float, degree=None, max_current=None, max_firing_rate=None):\n",
    "    if act_type == 'polynomial':\n",
    "        if degree is None or max_current is None or max_firing_rate is None:\n",
    "            raise ValueError(\"PolynomialActivation requires degree, max_current, and max_firing_rate\")\n",
    "        return PolynomialActivation(degree, max_current, max_firing_rate, bin_size)\n",
    "    elif act_type == 'relu':\n",
    "        return ReLUActivation(bin_size, max_current)\n",
    "    elif act_type == 'sigmoid':\n",
    "        if max_firing_rate is None:\n",
    "            raise ValueError(\"SigmoidActivation requires max_firing_rate\")\n",
    "        return SigmoidActivation(bin_size, max_firing_rate, max_current)\n",
    "    elif act_type == 'softplus':\n",
    "        return SoftplusActivation(bin_size, max_current)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation type: {act_type}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Fitting routines (with train/test split)\n",
    "# -----------------------------------------------------------------------------\n",
    "def fit_activation(\n",
    "    actv,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    Is_train,\n",
    "    fs_train,\n",
    "    Is_test=None,\n",
    "    fs_test=None,\n",
    "    epochs=1000\n",
    "):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for _ in range(epochs):\n",
    "        # training step\n",
    "        total_train = 0\n",
    "        for current, fr in zip(Is_train, fs_train):\n",
    "            current = current.reshape(1,1)\n",
    "            pred = actv(current)\n",
    "            loss = criterion(pred * actv.bin_size, fr.reshape(1,1) * actv.bin_size)\n",
    "            total_train += loss\n",
    "        optimizer.zero_grad()\n",
    "        total_train.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(total_train.item() / len(Is_train))\n",
    "        # evaluation on test set\n",
    "        if Is_test is not None and fs_test is not None:\n",
    "            with torch.no_grad():\n",
    "                total_test = 0\n",
    "                for current, fr in zip(Is_test, fs_test):\n",
    "                    current = current.reshape(1,1)\n",
    "                    pred = actv(current)\n",
    "                    loss = criterion(pred * actv.bin_size, fr.reshape(1,1) * actv.bin_size)\n",
    "                    total_test += loss\n",
    "                test_losses.append(total_test.item() / len(Is_test))\n",
    "    return train_losses, test_losses\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Activation fitting with optional test split\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_activations(\n",
    "    act_type,\n",
    "    Is_train,\n",
    "    fs_train,\n",
    "    bin_size,\n",
    "    epochs=1000,\n",
    "    device=None,\n",
    "    g=None,\n",
    "    degree=None,\n",
    "    max_firing_rate=None,\n",
    "    Is_test=None,\n",
    "    fs_test=None\n",
    "):\n",
    "    # init if needed\n",
    "    if g is None:\n",
    "        max_current = float(torch.max(torch.abs(torch.cat((Is_train, Is_test) if Is_test is not None else Is_train))))\n",
    "        g = make_activation(act_type, bin_size, degree, max_current, max_firing_rate)\n",
    "    g = g.to(device)\n",
    "    criterion = torch.nn.PoissonNLLLoss(log_input=False)\n",
    "    optimizer = torch.optim.Adam(g.parameters(), lr=0.05)\n",
    "    train_losses, test_losses = fit_activation(\n",
    "        g, criterion, optimizer,\n",
    "        Is_train, fs_train,\n",
    "        Is_test, fs_test,\n",
    "        epochs\n",
    "    )\n",
    "    return g, train_losses, test_losses\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Full model fitting including activation train/test split\n",
    "# -----------------------------------------------------------------------------\n",
    "def fit_model(\n",
    "    cell_id,\n",
    "    activation_bin_size,\n",
    "    activation_type,\n",
    "    max_firing_rate=None,\n",
    "    degree=None,\n",
    "    device=None,\n",
    "    g=None\n",
    "):\n",
    "    data = get_data(cell_id)\n",
    "    Is_np, fs_np = preprocess_data(data, activation_bin_size)\n",
    "    Is = torch.tensor(Is_np).to(device)\n",
    "    fs = torch.tensor(fs_np).to(device)\n",
    "\n",
    "    # split into train/test\n",
    "    n = len(Is)\n",
    "    gen = torch.Generator().manual_seed(42)\n",
    "    idx = torch.randperm(n, generator=gen)\n",
    "    train_size = int(0.8 * n)\n",
    "    train_idx, test_idx = idx[:train_size], idx[train_size:]\n",
    "    Is_train, fs_train = Is[train_idx], fs[train_idx]\n",
    "    Is_test, fs_test = Is[test_idx], fs[test_idx]\n",
    "\n",
    "    # fit activation\n",
    "    g, train_losses, test_losses = get_activations(\n",
    "        activation_type,\n",
    "        Is_train,\n",
    "        fs_train,\n",
    "        activation_bin_size,\n",
    "        epochs=1000,\n",
    "        device=device,\n",
    "        g=g,\n",
    "        degree=degree,\n",
    "        max_firing_rate=max_firing_rate,\n",
    "        Is_test=Is_test,\n",
    "        fs_test=fs_test\n",
    "    )\n",
    "    activation_params = g.get_params()\n",
    "\n",
    "    # return activations and their losses\n",
    "    return activation_params, train_losses, test_losses\n",
    "\n",
    "def summarize_losses(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with columns\n",
    "      ['cell_id', 'activation_bin_size', 'activation_type', 'train_loss', 'test_loss'],\n",
    "    returns a new DataFrame with the mean train_loss and test_loss grouped by\n",
    "    activation_type and activation_bin_size.\n",
    "    \"\"\"\n",
    "    # group and average\n",
    "    summary = (\n",
    "        df\n",
    "        .groupby(['activation_type', 'activation_bin_size'])[['train_loss','test_loss']]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c290b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1796 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cell 566517779: activation function has already been fitted.\n",
      "Skipping cell 486875162: activation function has already been fitted.\n",
      "Skipping cell 562651165: activation function has already been fitted.\n",
      "Skipping cell 526573598: activation function has already been fitted.\n",
      "Skipping cell 608108585: activation function has already been fitted.\n",
      "Skipping cell 584146987: activation function has already been fitted.\n",
      "Skipping cell 479993900: activation function has already been fitted.\n",
      "Skipping cell 569270348: activation function has already been fitted.\n",
      "Skipping cell 508911693: activation function has already been fitted.\n",
      "Skipping cell 517693519: activation function has already been fitted.\n",
      "Skipping cell 609435731: activation function has already been fitted.\n",
      "Skipping cell 479010903: activation function has already been fitted.\n",
      "Skipping cell 341442651: activation function has already been fitted.\n",
      "Skipping cell 530022494: activation function has already been fitted.\n",
      "Skipping cell 485245025: activation function has already been fitted.\n",
      "Skipping cell 501891177: activation function has already been fitted.\n",
      "Skipping cell 528638057: activation function has already been fitted.\n",
      "Skipping cell 512319604: activation function has already been fitted.\n",
      "Skipping cell 478888052: activation function has already been fitted.\n",
      "Skipping cell 476086391: activation function has already been fitted.\n",
      "Skipping cell 469803127: activation function has already been fitted.\n",
      "Skipping cell 486178939: activation function has already been fitted.\n",
      "Skipping cell 475848827: activation function has already been fitted.\n",
      "Skipping cell 586072188: activation function has already been fitted.\n",
      "Skipping cell 504635521: activation function has already been fitted.\n",
      "Skipping cell 520470657: activation function has already been fitted.\n",
      "Skipping cell 558211203: activation function has already been fitted.\n",
      "Skipping cell 484679812: activation function has already been fitted.\n",
      "Skipping cell 486776965: activation function has already been fitted.\n",
      "Skipping cell 569720969: activation function has already been fitted.\n",
      "Skipping cell 464191630: activation function has already been fitted.\n",
      "Skipping cell 478888083: activation function has already been fitted.\n",
      "Skipping cell 466378900: activation function has already been fitted.\n",
      "Skipping cell 575774870: activation function has already been fitted.\n",
      "Skipping cell 469614747: activation function has already been fitted.\n",
      "Skipping cell 475029659: activation function has already been fitted.\n",
      "Skipping cell 517693603: activation function has already been fitted.\n",
      "Skipping cell 500859045: activation function has already been fitted.\n",
      "Skipping cell 556687527: activation function has already been fitted.\n",
      "Skipping cell 502120615: activation function has already been fitted.\n",
      "Skipping cell 328876201: activation function has already been fitted.\n",
      "Skipping cell 487661754: activation function has already been fitted.\n",
      "Skipping cell 502366405: activation function has already been fitted.\n",
      "Skipping cell 563380422: activation function has already been fitted.\n",
      "Skipping cell 476741830: activation function has already been fitted.\n",
      "Skipping cell 520454345: activation function has already been fitted.\n",
      "Skipping cell 530055374: activation function has already been fitted.\n",
      "Skipping cell 326508751: activation function has already been fitted.\n",
      "Skipping cell 541540569: activation function has already been fitted.\n",
      "Skipping cell 565723357: activation function has already been fitted.\n",
      "Skipping cell 520347870: activation function has already been fitted.\n",
      "Skipping cell 482910438: activation function has already been fitted.\n",
      "Skipping cell 548421866: activation function has already been fitted.\n",
      "Skipping cell 489259263: activation function has already been fitted.\n",
      "Skipping cell 501940485: activation function has already been fitted.\n",
      "Skipping cell 541540615: activation function has already been fitted.\n",
      "Skipping cell 485835016: activation function has already been fitted.\n",
      "Skipping cell 486236426: activation function has already been fitted.\n",
      "Skipping cell 584671501: activation function has already been fitted.\n",
      "Skipping cell 369697038: activation function has already been fitted.\n",
      "Skipping cell 486940963: activation function has already been fitted.\n",
      "Skipping cell 485835055: activation function has already been fitted.\n",
      "Skipping cell 581058864: activation function has already been fitted.\n",
      "Skipping cell 580010290: activation function has already been fitted.\n",
      "Skipping cell 575299891: activation function has already been fitted.\n",
      "Skipping cell 570081589: activation function has already been fitted.\n",
      "Skipping cell 528015670: activation function has already been fitted.\n",
      "Skipping cell 580895033: activation function has already been fitted.\n",
      "Skipping cell 574038330: activation function has already been fitted.\n",
      "Skipping cell 537313600: activation function has already been fitted.\n",
      "Skipping cell 558612809: activation function has already been fitted.\n",
      "Skipping cell 569278799: activation function has already been fitted.\n",
      "Skipping cell 580624725: activation function has already been fitted.\n",
      "Skipping cell 563790168: activation function has already been fitted.\n",
      "Skipping cell 591626585: activation function has already been fitted.\n",
      "Skipping cell 521625948: activation function has already been fitted.\n",
      "Skipping cell 569958754: activation function has already been fitted.\n",
      "Skipping cell 476266853: activation function has already been fitted.\n",
      "Skipping cell 318808427: activation function has already been fitted.\n",
      "Skipping cell 507101551: activation function has already been fitted.\n",
      "Skipping cell 476619123: activation function has already been fitted.\n",
      "Skipping cell 565870964: activation function has already been fitted.\n",
      "Skipping cell 537256313: activation function has already been fitted.\n",
      "Skipping cell 566460800: activation function has already been fitted.\n",
      "Skipping cell 623436161: activation function has already been fitted.\n",
      "Skipping cell 505692551: activation function has already been fitted.\n",
      "Skipping cell 490733962: activation function has already been fitted.\n",
      "Skipping cell 586072464: activation function has already been fitted.\n",
      "Skipping cell 502260112: activation function has already been fitted.\n",
      "Skipping cell 478716304: activation function has already been fitted.\n",
      "Skipping cell 537182609: activation function has already been fitted.\n",
      "Skipping cell 562381210: activation function has already been fitted.\n",
      "Skipping cell 557679003: activation function has already been fitted.\n",
      "Skipping cell 528687520: activation function has already been fitted.\n",
      "Skipping cell 583836069: activation function has already been fitted.\n",
      "Skipping cell 520470950: activation function has already been fitted.\n",
      "Skipping cell 481001895: activation function has already been fitted.\n",
      "Skipping cell 565698986: activation function has already been fitted.\n",
      "Skipping cell 529826240: activation function has already been fitted.\n",
      "Skipping cell 481018305: activation function has already been fitted.\n",
      "Skipping cell 587096515: activation function has already been fitted.\n",
      "Skipping cell 581157315: activation function has already been fitted.\n",
      "Skipping cell 563028422: activation function has already been fitted.\n",
      "Skipping cell 487358945: activation function has already been fitted.\n",
      "Skipping cell 611623402: activation function has already been fitted.\n",
      "Skipping cell 643572206: activation function has already been fitted.\n",
      "Skipping cell 581403121: activation function has already been fitted.\n",
      "Skipping cell 566305266: activation function has already been fitted.\n",
      "Skipping cell 522723828: activation function has already been fitted.\n",
      "Skipping cell 476144118: activation function has already been fitted.\n",
      "Skipping cell 587776508: activation function has already been fitted.\n",
      "Skipping cell 490283519: activation function has already been fitted.\n",
      "Skipping cell 570589704: activation function has already been fitted.\n",
      "Skipping cell 397345294: activation function has already been fitted.\n",
      "Skipping cell 529908238: activation function has already been fitted.\n",
      "Skipping cell 562381326: activation function has already been fitted.\n",
      "Skipping cell 575201827: activation function has already been fitted.\n",
      "Skipping cell 535708196: activation function has already been fitted.\n",
      "Skipping cell 508420652: activation function has already been fitted.\n",
      "Skipping cell 528605746: activation function has already been fitted.\n",
      "Skipping cell 585908787: activation function has already been fitted.\n",
      "Skipping cell 585925172: activation function has already been fitted.\n",
      "Skipping cell 488677942: activation function has already been fitted.\n",
      "Skipping cell 537297470: activation function has already been fitted.\n",
      "Skipping cell 397353539: activation function has already been fitted.\n",
      "Skipping cell 508387918: activation function has already been fitted.\n",
      "Skipping cell 518750800: activation function has already been fitted.\n",
      "Skipping cell 485884503: activation function has already been fitted.\n",
      "Skipping cell 479150681: activation function has already been fitted.\n",
      "Skipping cell 539640410: activation function has already been fitted.\n",
      "Skipping cell 614777438: activation function has already been fitted.\n",
      "Skipping cell 558662244: activation function has already been fitted.\n",
      "Skipping cell 510706277: activation function has already been fitted.\n",
      "Skipping cell 513843815: activation function has already been fitted.\n",
      "Skipping cell 562381417: activation function has already been fitted.\n",
      "Skipping cell 488677994: activation function has already been fitted.\n",
      "Skipping cell 323838579: activation function has already been fitted.\n",
      "Skipping cell 488440436: activation function has already been fitted.\n",
      "Skipping cell 585859704: activation function has already been fitted.\n",
      "Skipping cell 488432250: activation function has already been fitted.\n",
      "Skipping cell 557261437: activation function has already been fitted.\n",
      "Skipping cell 322781822: activation function has already been fitted.\n",
      "Skipping cell 482435718: activation function has already been fitted.\n",
      "Skipping cell 313860745: activation function has already been fitted.\n",
      "Skipping cell 557859470: activation function has already been fitted.\n",
      "Skipping cell 562676370: activation function has already been fitted.\n",
      "Skipping cell 528016025: activation function has already been fitted.\n",
      "Skipping cell 324493977: activation function has already been fitted.\n",
      "Skipping cell 486146717: activation function has already been fitted.\n",
      "Skipping cell 583426723: activation function has already been fitted.\n",
      "Skipping cell 515949222: activation function has already been fitted.\n",
      "Skipping cell 330080937: activation function has already been fitted.\n",
      "Skipping cell 567763632: activation function has already been fitted.\n",
      "Skipping cell 580149939: activation function has already been fitted.\n",
      "Skipping cell 486941367: activation function has already been fitted.\n",
      "Skipping cell 354190013: activation function has already been fitted.\n",
      "Skipping cell 578929342: activation function has already been fitted.\n",
      "Skipping cell 569623233: activation function has already been fitted.\n",
      "Skipping cell 570098371: activation function has already been fitted.\n",
      "Skipping cell 572424901: activation function has already been fitted.\n",
      "Skipping cell 539542218: activation function has already been fitted.\n",
      "Skipping cell 586072783: activation function has already been fitted.\n",
      "Skipping cell 489308885: activation function has already been fitted.\n",
      "Skipping cell 582632160: activation function has already been fitted.\n",
      "Skipping cell 388874977: activation function has already been fitted.\n",
      "Skipping cell 593453795: activation function has already been fitted.\n",
      "Skipping cell 325903081: activation function has already been fitted.\n",
      "Skipping cell 485630699: activation function has already been fitted.\n",
      "Skipping cell 570589935: activation function has already been fitted.\n",
      "Skipping cell 571654895: activation function has already been fitted.\n",
      "Skipping cell 526951157: activation function has already been fitted.\n",
      "Skipping cell 545604343: activation function has already been fitted.\n",
      "Skipping cell 560579320: activation function has already been fitted.\n",
      "Skipping cell 557998843: activation function has already been fitted.\n",
      "Skipping cell 557859579: activation function has already been fitted.\n",
      "Skipping cell 396903166: activation function has already been fitted.\n",
      "Skipping cell 572375809: activation function has already been fitted.\n",
      "Skipping cell 568951556: activation function has already been fitted.\n",
      "Skipping cell 486146828: activation function has already been fitted.\n",
      "Skipping cell 522257166: activation function has already been fitted.\n",
      "Skipping cell 545227537: activation function has already been fitted.\n",
      "Skipping cell 396968721: activation function has already been fitted.\n",
      "Skipping cell 490259231: activation function has already been fitted.\n",
      "Skipping cell 488686369: activation function has already been fitted.\n",
      "Skipping cell 531342116: activation function has already been fitted.\n",
      "Skipping cell 487359276: activation function has already been fitted.\n",
      "Skipping cell 623960880: activation function has already been fitted.\n",
      "Skipping cell 562062132: activation function has already been fitted.\n",
      "Skipping cell 396903227: activation function has already been fitted.\n",
      "Skipping cell 589128508: activation function has already been fitted.\n",
      "Skipping cell 488448839: activation function has already been fitted.\n",
      "Skipping cell 322274121: activation function has already been fitted.\n",
      "Skipping cell 490267468: activation function has already been fitted.\n",
      "Skipping cell 441000782: activation function has already been fitted.\n",
      "Skipping cell 605889373: activation function has already been fitted.\n",
      "Skipping cell 565191521: activation function has already been fitted.\n",
      "Skipping cell 341459814: activation function has already been fitted.\n",
      "Skipping cell 572416871: activation function has already been fitted.\n",
      "Skipping cell 592577385: activation function has already been fitted.\n",
      "Skipping cell 490931049: activation function has already been fitted.\n",
      "Skipping cell 573105010: activation function has already been fitted.\n",
      "Skipping cell 569820026: activation function has already been fitted.\n",
      "Skipping cell 474637203: activation function has already been fitted.\n",
      "Skipping cell 570098593: activation function has already been fitted.\n",
      "Skipping cell 560817063: activation function has already been fitted.\n",
      "Skipping cell 530097064: activation function has already been fitted.\n",
      "Skipping cell 395830185: activation function has already been fitted.\n",
      "Skipping cell 505512874: activation function has already been fitted.\n",
      "Skipping cell 470098860: activation function has already been fitted.\n",
      "Skipping cell 502383543: activation function has already been fitted.\n",
      "Skipping cell 476455864: activation function has already been fitted.\n",
      "Skipping cell 477127614: activation function has already been fitted.\n",
      "Skipping cell 488473535: activation function has already been fitted.\n",
      "Skipping cell 502359001: activation function has already been fitted.\n",
      "Skipping cell 488711144: activation function has already been fitted.\n",
      "Skipping cell 529884137: activation function has already been fitted.\n",
      "Skipping cell 508421103: activation function has already been fitted.\n",
      "Skipping cell 526943222: activation function has already been fitted.\n",
      "Skipping cell 539640827: activation function has already been fitted.\n",
      "Skipping cell 569295876: activation function has already been fitted.\n",
      "Skipping cell 490382353: activation function has already been fitted.\n",
      "Skipping cell 367567889: activation function has already been fitted.\n",
      "Skipping cell 587064346: activation function has already been fitted.\n",
      "Skipping cell 542131227: activation function has already been fitted.\n",
      "Skipping cell 545612828: activation function has already been fitted.\n",
      "Skipping cell 480183339: activation function has already been fitted.\n",
      "Skipping cell 396608557: activation function has already been fitted.\n",
      "Skipping cell 325919795: activation function has already been fitted.\n",
      "Skipping cell 554443833: activation function has already been fitted.\n",
      "Skipping cell 605660220: activation function has already been fitted.\n",
      "Skipping cell 484770879: activation function has already been fitted.\n",
      "Skipping cell 598631490: activation function has already been fitted.\n",
      "Skipping cell 482772037: activation function has already been fitted.\n",
      "Skipping cell 477135941: activation function has already been fitted.\n",
      "Skipping cell 583148615: activation function has already been fitted.\n",
      "Skipping cell 609485897: activation function has already been fitted.\n",
      "Skipping cell 565462089: activation function has already been fitted.\n",
      "Skipping cell 623379534: activation function has already been fitted.\n",
      "Skipping cell 320455763: activation function has already been fitted.\n",
      "Skipping cell 497583188: activation function has already been fitted.\n",
      "Skipping cell 579626068: activation function has already been fitted.\n",
      "Skipping cell 569615443: activation function has already been fitted.\n",
      "Skipping cell 565863515: activation function has already been fitted.\n",
      "Skipping cell 567764060: activation function has already been fitted.\n",
      "Skipping cell 527869035: activation function has already been fitted.\n",
      "Skipping cell 501023852: activation function has already been fitted.\n",
      "Skipping cell 327566447: activation function has already been fitted.\n",
      "Skipping cell 539542640: activation function has already been fitted.\n",
      "Skipping cell 485934207: activation function has already been fitted.\n",
      "Skipping cell 583804033: activation function has already been fitted.\n",
      "Skipping cell 584664194: activation function has already been fitted.\n",
      "Skipping cell 558474375: activation function has already been fitted.\n",
      "Skipping cell 471819401: activation function has already been fitted.\n",
      "Skipping cell 489751692: activation function has already been fitted.\n",
      "Skipping cell 478110866: activation function has already been fitted.\n",
      "Skipping cell 578774163: activation function has already been fitted.\n",
      "Skipping cell 483099796: activation function has already been fitted.\n",
      "Skipping cell 555697303: activation function has already been fitted.\n",
      "Skipping cell 565871768: activation function has already been fitted.\n",
      "Skipping cell 482444444: activation function has already been fitted.\n",
      "Skipping cell 566559909: activation function has already been fitted.\n",
      "Skipping cell 320668841: activation function has already been fitted.\n",
      "Skipping cell 471131311: activation function has already been fitted.\n",
      "Skipping cell 520471727: activation function has already been fitted.\n",
      "Skipping cell 501736631: activation function has already been fitted.\n",
      "Skipping cell 477930684: activation function has already been fitted.\n",
      "Skipping cell 541566142: activation function has already been fitted.\n",
      "Skipping cell 579290303: activation function has already been fitted.\n",
      "Skipping cell 569263301: activation function has already been fitted.\n",
      "Skipping cell 569869510: activation function has already been fitted.\n",
      "Skipping cell 527942865: activation function has already been fitted.\n",
      "Skipping cell 576120017: activation function has already been fitted.\n",
      "Skipping cell 584271064: activation function has already been fitted.\n",
      "Skipping cell 569664731: activation function has already been fitted.\n",
      "Skipping cell 479225052: activation function has already been fitted.\n",
      "Skipping cell 593618144: activation function has already been fitted.\n",
      "Skipping cell 567731428: activation function has already been fitted.\n",
      "Skipping cell 583804142: activation function has already been fitted.\n",
      "Skipping cell 565871856: activation function has already been fitted.\n",
      "Skipping cell 585753841: activation function has already been fitted.\n",
      "Skipping cell 486352114: activation function has already been fitted.\n",
      "Skipping cell 479225080: activation function has already been fitted.\n",
      "Skipping cell 397354239: activation function has already been fitted.\n",
      "Skipping cell 500966656: activation function has already been fitted.\n",
      "Skipping cell 560678143: activation function has already been fitted.\n",
      "Skipping cell 523748610: activation function has already been fitted.\n",
      "Skipping cell 586073346: activation function has already been fitted.\n",
      "Skipping cell 541541635: activation function has already been fitted.\n",
      "Skipping cell 505808144: activation function has already been fitted.\n",
      "Skipping cell 556524816: activation function has already been fitted.\n",
      "Skipping cell 327648537: activation function has already been fitted.\n",
      "Skipping cell 515056927: activation function has already been fitted.\n",
      "Skipping cell 579462433: activation function has already been fitted.\n",
      "Skipping cell 563373346: activation function has already been fitted.\n",
      "Skipping cell 483018019: activation function has already been fitted.\n",
      "Skipping cell 476218657: activation function has already been fitted.\n",
      "Skipping cell 504120613: activation function has already been fitted.\n",
      "Skipping cell 479298854: activation function has already been fitted.\n",
      "Skipping cell 578938153: activation function has already been fitted.\n",
      "Skipping cell 555263276: activation function has already been fitted.\n",
      "Skipping cell 476054829: activation function has already been fitted.\n",
      "Skipping cell 576046388: activation function has already been fitted.\n",
      "Skipping cell 329557305: activation function has already been fitted.\n",
      "Skipping cell 469763389: activation function has already been fitted.\n",
      "Skipping cell 482493761: activation function has already been fitted.\n",
      "Skipping cell 491029832: activation function has already been fitted.\n",
      "Skipping cell 560579916: activation function has already been fitted.\n",
      "Skipping cell 475751758: activation function has already been fitted.\n",
      "Skipping cell 573121877: activation function has already been fitted.\n",
      "Skipping cell 516916566: activation function has already been fitted.\n",
      "Skipping cell 321906005: activation function has already been fitted.\n",
      "Skipping cell 564430168: activation function has already been fitted.\n",
      "Skipping cell 541566300: activation function has already been fitted.\n",
      "Skipping cell 483083617: data file does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 408/1796 [15:59<3:40:21,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cell 490718897: data file does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1796/1796 [4:10:05<00:00,  8.35s/it]  \n"
     ]
    }
   ],
   "source": [
    "# USER‑SET: list of cell IDs and your bin size\n",
    "# cell_ids = [313860745]\n",
    "activation_bin_sizes = [20, 100]\n",
    "actv_types = ['polynomial', 'relu', 'sigmoid', 'softplus']\n",
    "\n",
    "rows = []\n",
    "for cell_id in tqdm(cell_ids):\n",
    "    path = config[\"data_path\"] + f\"processed_I_and_firing_rate_{cell_id}.pickle\"\n",
    "    path2 = \"data/activation_function/\" + f\"{cell_id}.pickle\"\n",
    "    # check if the data file does not exist and activation function has not been fitted\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Skipping cell {cell_id}: data file does not exist.\")\n",
    "    elif os.path.exists(path2):\n",
    "        print(f\"Skipping cell {cell_id}: activation function has already been fitted.\")\n",
    "        # load cell data and add to dataframe\n",
    "        with open(path2, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        rows.append({\n",
    "            'cell_id': cell_id,\n",
    "            'activation_bin_size': data['activation_bin_size'],\n",
    "            'activation_type': data['activation_type'],\n",
    "            'train_loss': data['train_loss'],\n",
    "            'test_loss': data['test_loss'],\n",
    "            \"activation_params\": data['activation_params']\n",
    "        })\n",
    "    else:\n",
    "        # load that cell’s max firing rate\n",
    "        with open(f\"{config['mfr_path']}{cell_id}.pickle\", \"rb\") as f:\n",
    "            max_firing_rate = pickle.load(f)\n",
    "\n",
    "        for act_type in actv_types:\n",
    "            for activation_bin_size in activation_bin_sizes:\n",
    "                # fit_model returns (activation_params, train_losses, test_losses)\n",
    "                activation_params, train_losses, test_losses = fit_model(\n",
    "                    cell_id,\n",
    "                    activation_bin_size,\n",
    "                    act_type,\n",
    "                    max_firing_rate,\n",
    "                    degree=2\n",
    "                )\n",
    "\n",
    "                rows.append({\n",
    "                    'cell_id': cell_id,\n",
    "                    'activation_bin_size': activation_bin_size,\n",
    "                    'activation_type': act_type,\n",
    "                    'train_loss': train_losses[-1],\n",
    "                    'test_loss':  test_losses[-1],\n",
    "                    \"activation_params\": activation_params\n",
    "                })\n",
    "\n",
    "                # save data as data/activation_function/[cell_id].pickle\n",
    "                with open(f\"data/activation_function/{cell_id}.pickle\", \"wb\") as f:\n",
    "                    pickle.dump({\n",
    "                        'activation_bin_size': activation_bin_size,\n",
    "                        'activation_type': act_type,\n",
    "                        'train_loss': train_losses[-1],\n",
    "                        'test_loss':  test_losses[-1],\n",
    "                        \"activation_params\": activation_params\n",
    "                    }, f)\n",
    "\n",
    "# build & inspect\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# save dataframe as pickle\n",
    "with open(\"model/activation_fits.pickle\", \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45822758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  activation_type  activation_bin_size  train_loss  test_loss\n",
      "0      polynomial                   20    0.207010   0.314541\n",
      "1      polynomial                  100   -0.480744  -0.278284\n",
      "2            relu                   20    0.348153   0.480723\n",
      "3            relu                  100    1.018951   1.374021\n",
      "4         sigmoid                   20    1.015814   1.087993\n",
      "5         sigmoid                  100    3.427711   3.648883\n",
      "6        softplus                   20    7.577573   7.980943\n",
      "7        softplus                  100   35.628174  37.570494\n"
     ]
    }
   ],
   "source": [
    "summary_df = summarize_losses(df)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f1a1121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  activation_type  activation_bin_size  train_loss  test_loss\n",
      "0      polynomial                   20    0.207010   0.314541\n",
      "1      polynomial                  100   -0.480744  -0.278284\n",
      "2            relu                   20    0.348153   0.480723\n",
      "3            relu                  100    1.018951   1.374021\n",
      "4         sigmoid                   20    1.015814   1.087993\n",
      "5         sigmoid                  100    3.427711   3.648883\n",
      "6        softplus                   20    7.577573   7.980943\n",
      "7        softplus                  100   35.628174  37.570494\n"
     ]
    }
   ],
   "source": [
    "# drop nan rows in df\n",
    "df_no_nan = df.dropna(subset=['train_loss', 'test_loss'])\n",
    "summary_no_nan = summarize_losses(df_no_nan)\n",
    "print(summary_no_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e661e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_no_nan: 12161\n",
      "Number of rows in df: 12161\n"
     ]
    }
   ],
   "source": [
    "# number of rows in df_no_nan\n",
    "print(f\"Number of rows in df_no_nan: {len(df_no_nan)}\")\n",
    "\n",
    "# number of rows in df\n",
    "print(f\"Number of rows in df: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a2cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
