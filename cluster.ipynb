{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98bc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from model import GFR\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(params, threshold=0.5):\n",
    "    with open(\"data/labels.pickle\", \"rb\") as f:\n",
    "        labels = pickle.load(f)\n",
    "    \n",
    "    chosen_ids = filter(lambda x: params[x][\"evr2\"] > threshold, params.keys())\n",
    "    \n",
    "    dataset = {}\n",
    "    for cell_id in chosen_ids:\n",
    "        y = labels[cell_id]\n",
    "        p = params[cell_id][\"params\"]\n",
    "        \n",
    "        a = p[\"a\"].reshape(-1)\n",
    "        b = p[\"b\"].reshape(-1)\n",
    "        pc = p[\"g\"][\"poly_coeff\"].reshape(-1)\n",
    "        gb = p[\"g\"][\"b\"].reshape(-1)\n",
    "        mc = p[\"g\"][\"max_current\"].reshape(-1)\n",
    "        mfr = p[\"g\"][\"max_firing_rate\"].reshape(-1)\n",
    "        x = torch.cat([a, b, pc, gb, mc, mfr])\n",
    "        \n",
    "        dataset[cell_id] = (x, y, params[cell_id][\"evr2\"])\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def get_line_name(df, cell_id):\n",
    "    return df[df[\"specimen__id\"] == cell_id][\"line_name\"].to_numpy()[0]\n",
    "\n",
    "def get_labels(cell_ids):\n",
    "    df = pd.read_csv(\"data/metadata.csv\")\n",
    "    line_names = [\"Pvalb\", \"Sst\", \"Vip\", \"Htr3a\", \"Ndnf\", \"Cux2\", \"Nr5a1\", \"Ntsr1\", \"Rorb\", \"Scnn1a\", \"Tlx3\", \"Rbp4\"]\n",
    "    new_cell_ids = []\n",
    "    labels = []\n",
    "    \n",
    "    for cell_id in cell_ids:\n",
    "        line_name = get_line_name(df, cell_id)\n",
    "        if type(line_name) != str or \"|\" in line_name:\n",
    "            pass\n",
    "        elif line_name.split(\"-\")[0] in line_names:\n",
    "            new_cell_ids.append(cell_id)\n",
    "            labels.append(line_name.split(\"-\")[0])\n",
    "    \n",
    "    types, counts = np.unique(labels, return_counts=True)\n",
    "    types = types[counts > 20]\n",
    "    new_cell_ids, labels = zip(*list(filter(lambda x: x[1] in types, zip(new_cell_ids, labels))))\n",
    "    \n",
    "    return np.array(new_cell_ids), np.array(labels)\n",
    "\n",
    "def get_dataset(dataset=None, instant=False):\n",
    "    if dataset is None:\n",
    "        with open(\"data/dataset.pickle\", \"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "    cell_ids, ys = get_labels(dataset.keys())\n",
    "    #remove = []#[479298854, 489719193, 485060618, 486791945, 562535995, 321707905]# + [478110866]\n",
    "    #idxs = list(filter(lambda x: cell_ids[x] in remove, range(len(cell_ids))))\n",
    "    #cell_ids = [cell_ids[i] for i in range(len(cell_ids)) if i not in idxs]\n",
    "    #ys = [ys[i] for i in range(len(ys)) if i not in idxs]\n",
    "    xs = []\n",
    "    for cell_id in cell_ids:\n",
    "        x = dataset[cell_id][0].tolist()\n",
    "        c0 = (x[-5]**2 - x[-3]) / x[-2]\n",
    "        c1 = x[-4]**2 / x[-2]\n",
    "        if not instant:\n",
    "            xs.append(x[:-5] + [c0, c1, x[-1]])\n",
    "        else:\n",
    "            xs.append(x[8:-5] + [c0, c1, x[-1]])\n",
    "    xs = np.array(xs)\n",
    "    return xs, ys, cell_ids\n",
    "\n",
    "def get_best_params_for_actv_bin_size(params, bin_size, actv_bin_size):\n",
    "    best_params = {}\n",
    "    \n",
    "    cell_ids = set()\n",
    "    for config in params:\n",
    "        cell_ids = cell_ids.union(set(params[config].keys()))\n",
    "    \n",
    "    for cell_id in cell_ids:\n",
    "        best_config = None\n",
    "        best_evr = -1e10\n",
    "        \n",
    "        for config in params:\n",
    "            if config[0] == bin_size and config[1] == actv_bin_size and cell_id in params[config] and params[config][cell_id][\"evr1\"] > best_evr:\n",
    "                best_evr = params[config][cell_id][\"evr1\"]\n",
    "                best_config = config\n",
    "        \n",
    "        # doesn't make sense\n",
    "        if best_config is not None:\n",
    "            best_params[cell_id] = params[best_config][cell_id]\n",
    "        \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e66be-0be5-4fec-9e04-7e548659707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torcheval.metrics.functional import multiclass_f1_score, multiclass_accuracy, multiclass_confusion_matrix\n",
    "\n",
    "def summary(model, dataloader, n_classes):\n",
    "    y_pred, y = get_predictions(model, dataloader, n_classes)\n",
    "    f = multiclass_f1_score(y_pred, y, num_classes=n_classes, average=\"weighted\")\n",
    "    acc = multiclass_accuracy(y_pred, y, num_classes=n_classes)\n",
    "    return f, acc\n",
    "\n",
    "def get_predictions(model, dataloader, n_classes):\n",
    "    ys, ys_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y, w in dataloader:\n",
    "            X = X.to(torch.float32)\n",
    "            y1 = torch.einsum(\"ij,j->i\", y, torch.arange(n_classes))\n",
    "            y = y.to(torch.float32)\n",
    "            ys_pred.append(model(X))\n",
    "            ys.append(y1)\n",
    "    return torch.cat(ys_pred), torch.cat(ys)\n",
    "\n",
    "class ParameterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, xs, ys, label_map):\n",
    "        self.n_classes = len(label_map)\n",
    "        self.xs = list(map(torch.tensor, xs))\n",
    "        self.ys = list(map(torch.tensor, [label_map[y] for y in ys]))\n",
    "        self.ys = list(map(lambda x: F.one_hot(x, num_classes=self.n_classes), self.ys))\n",
    "        \n",
    "        types, counts = np.unique(ys, return_counts=True)\n",
    "        ws = (1 / counts) / np.sum(1 / counts)\n",
    "        w_map = {t: w for t, w in zip(types, ws)}\n",
    "        self.ws = list(map(lambda x: w_map[x], ys))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx], self.ws[idx]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_model(xs, ys, pca=True, scale=True, linear=False):\n",
    "    types = np.unique(ys).tolist()\n",
    "    label_map = {t: i for i, t in enumerate(types)}\n",
    "    n_classes = len(label_map)\n",
    "\n",
    "    z = list(zip(xs, ys))\n",
    "    ztr, zte = train_test_split(z, test_size=0.35, stratify=ys)\n",
    "    Xtr, ytr = zip(*ztr)\n",
    "    Xte, yte = zip(*zte)\n",
    "    z = list(zip(Xtr, ytr))\n",
    "    ztr, zval = train_test_split(z, test_size=0.15, stratify=ytr)\n",
    "    Xtr, ytr = zip(*ztr)\n",
    "    Xval, yval = zip(*zval)\n",
    "    \n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(Xtr)\n",
    "        Xtr = scaler.transform(Xtr)\n",
    "        Xval = scaler.transform(Xval)\n",
    "        Xte = scaler.transform(Xte)\n",
    "        xs = scaler.transform(xs)\n",
    "    \n",
    "    if pca:\n",
    "        scaler = PCA(n_components=5)\n",
    "        scaler.fit(Xtr)\n",
    "        Xtr = scaler.transform(Xtr)\n",
    "        Xval = scaler.transform(Xval)\n",
    "        Xte = scaler.transform(Xte)\n",
    "        xs = scaler.transform(xs)\n",
    "\n",
    "    train_dataset = ParameterDataset(Xtr, ytr, label_map)\n",
    "    val_dataset = ParameterDataset(Xval, yval, label_map)\n",
    "    test_dataset = ParameterDataset(Xte, yte, label_map)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(yte), shuffle=False)\n",
    "    \n",
    "    if linear:\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(len(xs[0]), n_classes)\n",
    "        )\n",
    "    else:\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(len(xs[0]), 10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10, n_classes)\n",
    "        )\n",
    "\n",
    "    types, counts = np.unique(ys, return_counts=True)\n",
    "    ws = torch.tensor((1 / counts) / np.sum(1 / counts))\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=ws)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    epochs = 500\n",
    "\n",
    "    train_accs = []\n",
    "    train_f1s = []\n",
    "    val_accs = []\n",
    "    val_f1s = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for X, y, w in train_loader:\n",
    "            X = X.to(torch.float32)\n",
    "            y = y.to(torch.float32)\n",
    "            y_pred = model(X)\n",
    "            loss += criterion(y_pred, y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        f_tr, acc_tr = summary(model, train_loader, n_classes)\n",
    "        f_val, acc_val = summary(model, val_loader, n_classes)\n",
    "        train_accs.append(acc_tr.item())\n",
    "        train_f1s.append(f_tr.item())\n",
    "        val_accs.append(acc_val.item())\n",
    "        val_f1s.append(f_val.item())\n",
    "\n",
    "        if (epoch + 1) % 10000 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item()}\")\n",
    "\n",
    "    #print_summary(model, train_loader, n_classes)\n",
    "    f, acc = summary(model, test_loader, n_classes)\n",
    "        \n",
    "    return model, f.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71769a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x, a):\n",
    "    ds = torch.tensor([1.0000, 0.6321, 0.3297, 0.1813, 0.0952, 0.0392, 0.0198, 0.0100])\n",
    "    return torch.einsum(\"ij,j->i\", torch.tensor(a).to(torch.float32), torch.pow(1 - ds, torch.tensor(x)))\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    \n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b5314-b417-4ed8-aa92-48ab3c867cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/best_params.pickle\", \"rb\") as f:\n",
    "    all_params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e7807-5af6-4f38-b861-a8e042fc5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "instant = False\n",
    "bin_size = 20\n",
    "actv_bin_size = 20\n",
    "#params = get_best_params_for_actv_bin_size(all_params, bin_size, actv_bin_size)\n",
    "dataset = gen_dataset(all_params[(bin_size, actv_bin_size)])\n",
    "xs, ys, cell_ids = get_dataset(dataset, instant)\n",
    "ys = np.array(ys)\n",
    "\n",
    "# !!!!!!!!!!!!!!!\n",
    "types, counts = np.unique(ys, return_counts=True)\n",
    "if instant:\n",
    "    types = [\"Htr3a\", \"Ndnf\", \"Vip\", \"Sst\", \"Pvalb\", \"Cux2\", \"Nr5a1\", \"Scnn1a\", \"Rorb\", \"Ntsr1\"]#, \"Rbp4\"]\n",
    "else:\n",
    "    types = [\"Htr3a\", \"Ndnf\", \"Vip\", \"Sst\", \"Pvalb\", \"Cux2\", \"Nr5a1\", \"Scnn1a\", \"Rorb\", \"Ntsr1\", \"Rbp4\"]\n",
    "inhibitory = [\"Pvalb\", \"Sst\", \"Vip\", \"Htr3a\", \"Ndnf\"]\n",
    "# !!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb910e-b04e-45ca-a9e0-fd0076ade85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsss = []\n",
    "for binary in [True, False]:\n",
    "    print(f\"{binary=}\")\n",
    "    if binary:\n",
    "        labels = [\"inhibitory\" if y in inhibitory else \"excitatory\" for y in ys]\n",
    "    else:\n",
    "        labels = ys\n",
    "\n",
    "    fs = []\n",
    "    accs = []\n",
    "    wss = []\n",
    "    for i in range(30):\n",
    "        model, f, acc = train_model(xs, labels, pca=False, scale=True, linear=True)\n",
    "        fs.append(f)\n",
    "        accs.append(acc)\n",
    "        ws = model[0].weight.detach().norm(dim=0)\n",
    "        ws = ws / torch.sum(ws)\n",
    "        wss.append(ws)\n",
    "    wsss.append(wss)\n",
    "    \n",
    "    print(f\"({binary=}) f1 score: mean {np.mean(fs):.4f}, std {np.std(fs):.4f}\")\n",
    "    print(f\"({binary=}) accuracy: mean {np.mean(accs):.4f}, std {np.std(accs):.4f}\")\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsss[0] = torch.stack(wsss[0])\n",
    "wsss[1] = torch.stack(wsss[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe75fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsss[0].std(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e73d58-a1ff-40f0-8d87-0c9e8574ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(7.5, 4.5), dpi=1000)\n",
    "fig.text(-0.03, 0.5, 'relative importance', va='center', rotation='vertical')\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "\n",
    "if not instant:\n",
    "    alpha = [f\"$\\\\alpha_{i+1}$\" for i in range(8)]\n",
    "    beta = [f\"$\\\\beta_{i+1}$\" for i in range(8)]\n",
    "    #actv = [f\"$a_{i}$\" for i in range(4 if actv_bin_size > 20 else 2)] + [\"b\", \"max_current\", \"max_firing_rate\"]\n",
    "    actv = [\"$c_0$\", \"$c_1$\", \"$\\\\gamma$\"]\n",
    "    labels = alpha + beta + actv\n",
    "    ax1.bar(labels, wsss[0].mean(axis=0), yerr=wsss[0].std(axis=0), color=([\"tab:green\"]*8+[\"tab:purple\"]*8+[\"lightblue\"]*len(actv)))\n",
    "    ax2.bar(labels, wsss[1].mean(axis=0), yerr=wsss[1].std(axis=0), color=([\"tab:green\"]*8+[\"tab:purple\"]*8+[\"lightblue\"]*len(actv)))\n",
    "    ax1.set_xticks([])\n",
    "    fig.tight_layout()\n",
    "else:\n",
    "    beta = [f\"$\\\\beta_{i+1}$\" for i in range(8)]\n",
    "    #actv = [f\"$a_{i}$\" for i in range(4 if actv_bin_size > 20 else 2)] + [\"b\", \"max_current\", \"max_firing_rate\"]\n",
    "    actv = [\"$c_0$\", \"$c_1$\", \"$\\\\gamma$\"]\n",
    "    labels = beta + actv\n",
    "    ax1.bar(labels, wsss[0].mean(axis=0), yerr=wsss[0].std(axis=0), color=([\"tab:purple\"]*8+[\"lightblue\"]*len(actv)))\n",
    "    ax2.bar(labels, wsss[1].mean(axis=0), yerr=wsss[1].std(axis=0), color=([\"tab:purple\"]*8+[\"lightblue\"]*len(actv)))\n",
    "    ax1.set_xticks([])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a49b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {t: i for i, t in enumerate(types)}\n",
    "inv_mapping = {i: t for i, t in enumerate(types)}\n",
    "\n",
    "xs_scaled = StandardScaler().fit_transform(xs)\n",
    "\n",
    "n_clusters = 10 if instant else 11\n",
    "clustering = AgglomerativeClustering(n_clusters=n_clusters, compute_distances=True, metric=\"euclidean\", linkage=\"ward\")\n",
    "clustering.fit(xs_scaled)\n",
    "\n",
    "plot_dendrogram(clustering, truncate_mode=\"lastp\", p=n_clusters, leaf_rotation=60.,\n",
    "            leaf_font_size=12.)\n",
    "print(np.unique(clustering.labels_, return_counts=True))\n",
    "\n",
    "id1 = []\n",
    "id2 = []\n",
    "mat = np.zeros((n_clusters, len(types)))\n",
    "for i, y, x, cell_id in zip(clustering.labels_, ys, xs, cell_ids):\n",
    "    #if i == 9:# and y == \"Vip\":\n",
    "    #    print(cell_id, x)\n",
    "    mat[i, mapping[y]] += 1\n",
    "\n",
    "idx_map = [(j, i) for i, j in enumerate(sorted(range(mat.shape[0]), key=lambda i: mat[i, :] @ np.array([-1 if x < 5 else 1 for x in range(len(types))])))]\n",
    "perm = np.zeros((n_clusters, n_clusters))\n",
    "for i, j in idx_map:\n",
    "    perm[j, i] = 1\n",
    "mat = perm @ mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.collections import PatchCollection\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "N, M = mat.shape\n",
    "ylabels = [f\"Cluster {i+1}\" for i in range(n_clusters)][::-1]\n",
    "xlabels = [f\"{inv_mapping[i]}\" for i in range(len(types))]\n",
    "\n",
    "x, y = np.meshgrid(np.arange(M), np.arange(N))\n",
    "s = mat / mat.mean(axis=0)\n",
    "c = np.zeros((N, M))\n",
    "norm = mat / mat.max()\n",
    "c[:, :5] = -2 - norm[:,:5]\n",
    "c[:, 5:] = 2 + norm[:,5:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5 * N / M), dpi=1000)\n",
    "\n",
    "#R = s/s.max()/2.2\n",
    "R = normalize(s, norm='l2', axis=0) / 2.2\n",
    "circles = [plt.Circle((j, i), radius=r, color='k', linewidth=10) for r, j, i in zip(R.flat, x.flat, y.flat)]\n",
    "col = PatchCollection(circles, array=c.flatten(), cmap=\"bwr\")\n",
    "ax.add_collection(col)\n",
    "\n",
    "ax.set(xticks=np.arange(M), yticks=np.arange(N),\n",
    "       xticklabels=xlabels, yticklabels=ylabels)\n",
    "ax.set_xticks(np.arange(M+1)-0.5, minor=True)\n",
    "ax.set_yticks(np.arange(N+1)-0.5, minor=True)\n",
    "ax.grid()\n",
    "ax.set_axisbelow(True)\n",
    "plt.xticks(rotation=90)\n",
    "#fig.colorbar(col)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde8ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "for x in [xs_scaled]:\n",
    "    clustering = AgglomerativeClustering(n_clusters=len(types), compute_distances=True, metric=\"euclidean\", linkage=\"ward\")\n",
    "    clustering.fit(x);\n",
    "    #print(sklearn.metrics.v_measure_score(ys, clustering.labels_))\n",
    "    print(sklearn.metrics.adjusted_rand_score(ys, clustering.labels_))\n",
    "    print(sklearn.metrics.v_measure_score(ys, clustering.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9972f-7d1b-4caf-8e04-705c39b7a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = gen_dataset(all_params[(bin_size, actv_bin_size)])\n",
    "xs, ys, cell_ids = get_dataset(dataset, instant)\n",
    "ys = np.array(ys)\n",
    "types, counts = np.unique(ys, return_counts=True)\n",
    "n_clusters = len(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = [\"Htr3a\", \"Ndnf\", \"Vip\", \"Sst\", \"Pvalb\", \"Cux2\", \"Nr5a1\", \"Scnn1a\", \"Rorb\", \"Ntsr1\", \"Rbp4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcfbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(types), 4, figsize=(12, 15), dpi=250)\n",
    "for i, t in enumerate(types):\n",
    "    a = xs[ys == t, :8]\n",
    "    ax[i, 0].violinplot(a)\n",
    "    ax[i, 0].xaxis.set_ticks(ticks=range(1, 9), labels=[str(j) for j in [0, 20, 50, 100, 200, 500, 1000, 2000]], rotation=90)\n",
    "    \n",
    "    zs = torch.linspace(0, 10, 500)\n",
    "    X = torch.stack([kernel(z, a) for z in zs])\n",
    "    x_mean = torch.mean(X, dim=1)\n",
    "    x_std = torch.std(X, dim=1)\n",
    "    \n",
    "    \n",
    "    ax[i, 1].plot(zs, x_mean)\n",
    "    ax[i, 1].fill_between(zs, x_mean - x_std, x_mean + x_std, alpha=0.1, color=\"tab:blue\")\n",
    "    \n",
    "    \n",
    "    b = xs[ys == t, 8:16]\n",
    "    ax[i, 2].violinplot(b)\n",
    "    ax[i, 2].xaxis.set_ticks(ticks=range(1, 9), labels=[str(j) for j in [0, 20, 50, 100, 200, 500, 1000, 2000]], rotation=90)\n",
    "    \n",
    "    zs = torch.linspace(0, 50, 500)\n",
    "    X = torch.stack([kernel(z, b) for z in zs])\n",
    "    x_mean = torch.mean(X, dim=1)\n",
    "    x_std = torch.std(X, dim=1)\n",
    "    \n",
    "    \n",
    "    ax[i, 3].plot(zs, x_mean)\n",
    "    ax[i, 3].fill_between(zs, x_mean - x_std, x_mean + x_std, alpha=0.1, color=\"tab:blue\")\n",
    "    \n",
    "    ax[i, 0].set_ylabel(t)\n",
    "    ax[i, 0].set_ylim([-0.5, 1])\n",
    "    ax[i, 1].set_ylim([-0.2, 1.7])\n",
    "    ax[i, 2].set_ylim([-2.5, 1.8])\n",
    "    ax[i, 3].set_ylim([-5, 0.1])\n",
    "    \n",
    "    if i == 0:\n",
    "        ax[i, 0].set_title(\"$\\\\alpha_i$\")\n",
    "        ax[i, 1].set_title(\"$k_I(t)$\")\n",
    "        ax[i, 2].set_title(\"$\\\\beta_i$\")\n",
    "        ax[i, 3].set_title(\"$k_f(t)$\")\n",
    "    \n",
    "    if i < len(types)-1:\n",
    "        for j in range(4):\n",
    "            ax[i, j].xaxis.set_ticklabels([])\n",
    "            \n",
    "    if i == len(types) - 1:\n",
    "        ax[i, 0].set_xlabel(\"$\\\\tau$\")\n",
    "        ax[i, 1].set_xlabel(\"$t$ (s)\")\n",
    "        ax[i, 2].set_xlabel(\"$\\\\tau$\")\n",
    "        ax[i, 3].set_xlabel(\"$t$ (s)\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a856d-8627-45d9-82b2-bab121d451fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_map_inv = perm.argmax(axis=1)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27509802-6c27-470e-8f25-bd7a15d9925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_labels = clustering.labels_\n",
    "fig, ax = plt.subplots(n_clusters, 4, figsize=(12, 15), dpi=250)\n",
    "for i in range(n_clusters):\n",
    "    t = idx_map_inv[i]\n",
    "    a = xs[clustering_labels == t, :8]\n",
    "    ax[i, 0].violinplot(a)\n",
    "    ax[i, 0].xaxis.set_ticks(ticks=range(1, 9), labels=[str(j) for j in [0, 20, 50, 100, 200, 500, 1000, 2000]], rotation=90)\n",
    "    \n",
    "    zs = torch.linspace(0, 10, 500)\n",
    "    X = torch.stack([kernel(z, a) for z in zs])\n",
    "    x_mean = torch.mean(X, dim=1)\n",
    "    x_std = torch.std(X, dim=1)\n",
    "    \n",
    "    \n",
    "    ax[i, 1].plot(zs, x_mean)\n",
    "    ax[i, 1].fill_between(zs, x_mean - x_std, x_mean + x_std, alpha=0.1, color=\"tab:blue\")\n",
    "    \n",
    "    \n",
    "    b = xs[clustering_labels == t, 8:16]\n",
    "    ax[i, 2].violinplot(b)\n",
    "    ax[i, 2].xaxis.set_ticks(ticks=range(1, 9), labels=[str(j) for j in [0, 20, 50, 100, 200, 500, 1000, 2000]], rotation=90)\n",
    "    \n",
    "    zs = torch.linspace(0, 50, 500)\n",
    "    X = torch.stack([kernel(z, b) for z in zs])\n",
    "    x_mean = torch.mean(X, dim=1)\n",
    "    x_std = torch.std(X, dim=1)\n",
    "    \n",
    "    \n",
    "    ax[i, 3].plot(zs, x_mean)\n",
    "    ax[i, 3].fill_between(zs, x_mean - x_std, x_mean + x_std, alpha=0.1, color=\"tab:blue\")\n",
    "    \n",
    "    ax[i, 0].set_ylabel(f\"Cluster {i+1}\")\n",
    "    ax[i, 0].set_ylim([-0.5, 0.7])\n",
    "    ax[i, 1].set_ylim([-0.2, 2.3])\n",
    "    ax[i, 2].set_ylim([-2.5, 1.8])\n",
    "    ax[i, 3].set_ylim([-5, 0.1])\n",
    "    \n",
    "    if i == 0:\n",
    "        ax[i, 0].set_title(\"$\\\\alpha_i$\")\n",
    "        ax[i, 1].set_title(\"$k_I(t)$\")\n",
    "        ax[i, 2].set_title(\"$\\\\beta_i$\")\n",
    "        ax[i, 3].set_title(\"$k_f(t)$\")\n",
    "    \n",
    "    if i < n_clusters-1:\n",
    "        for j in range(4):\n",
    "            ax[i, j].xaxis.set_ticklabels([])\n",
    "            \n",
    "    if i == n_clusters - 1:\n",
    "        ax[i, 0].set_xlabel(\"$\\\\tau$\")\n",
    "        ax[i, 1].set_xlabel(\"$t$ (s)\")\n",
    "        ax[i, 2].set_xlabel(\"$\\\\tau$\")\n",
    "        ax[i, 3].set_xlabel(\"$t$ (s)\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3175b3b-611d-4388-898c-f4c507988e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_labels = clustering.labels_\n",
    "fig, ax = plt.subplots(4, 1, figsize=(8, 6), dpi=500)\n",
    "\n",
    "for k, j in enumerate([0, 16, 17, 18]):\n",
    "    d = []\n",
    "    for t in types:\n",
    "        d.append(xs[ys == t, j])\n",
    "\n",
    "    #ax[k].violinplot(d, quantiles=[[0.25, 0.5, 0.75] for i in range(n_clusters)])\n",
    "    ax[k].violinplot(d)\n",
    "    if k < 3:\n",
    "        ax[k].xaxis.set_ticks(ticks=list(range(1, len(types)+1)), labels=[\"\" for i in range(len(types))])\n",
    "    else:\n",
    "        ax[k].xaxis.set_ticks(ticks=list(range(1, len(types)+1)), labels=types)\n",
    "\n",
    "ax[0].set_ylabel(\"$\\\\alpha_1$\")\n",
    "ax[1].set_ylabel(\"$c_0$\")\n",
    "ax[2].set_ylabel(\"$c_1$\")\n",
    "ax[3].set_ylabel(\"$\\\\gamma$\")\n",
    "ax[3].set_xlabel(\"Cre-line\")\n",
    "\n",
    "ax[0].set_ylim([6.8, 9])\n",
    "ax[1].set_ylim([-1.1, 0])\n",
    "ax[2].set_ylim([-0.01, 0.16])\n",
    "ax[3].set_ylim([0, 0.7])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26aeb2-4a1b-4a82-b5c2-bef11bb01a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_labels = clustering.labels_\n",
    "fig, ax = plt.subplots(4, 1, figsize=(8, 6), dpi=500)\n",
    "\n",
    "for k, j in enumerate([0, 16, 17, 18]):\n",
    "    d = []\n",
    "    for i in range(n_clusters):\n",
    "        t = idx_map_inv[i]\n",
    "        d.append(xs[clustering_labels == t, j])\n",
    "\n",
    "    #ax[k].violinplot(d, quantiles=[[0.25, 0.5, 0.75] for i in range(n_clusters)])\n",
    "    ax[k].violinplot(d)\n",
    "    if k < 3:\n",
    "        ax[k].xaxis.set_ticks(ticks=list(range(1, n_clusters+1)), labels=[\"\" for i in range(n_clusters)])\n",
    "    else:\n",
    "        ax[k].xaxis.set_ticks(ticks=list(range(1, n_clusters+1)), labels=[f\"{i+1}\" for i in range(n_clusters)])\n",
    "\n",
    "ax[0].set_ylabel(\"$\\\\alpha_1$\")\n",
    "ax[1].set_ylabel(\"$c_0$\")\n",
    "ax[2].set_ylabel(\"$c_1$\")\n",
    "ax[3].set_ylabel(\"$\\\\gamma$\")\n",
    "ax[3].set_xlabel(\"Cluster\")\n",
    "\n",
    "ax[0].set_ylim([6.5, 8.8])\n",
    "ax[1].set_ylim([-1, -0.05])\n",
    "ax[2].set_ylim([-0.01, 0.23])\n",
    "ax[3].set_ylim([0, 0.7])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada70a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
