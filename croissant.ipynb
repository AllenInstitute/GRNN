{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "\n",
    "# FileObjects and FileSets define the resources of the dataset.\n",
    "distribution = [\n",
    "    # gpt-3 is hosted on a GitHub repository:\n",
    "    mlc.FileObject(\n",
    "        id=\"github-repository\",\n",
    "        name=\"github-repository\",\n",
    "        description=\"Generalized Firing Rate Neurons repository on GitHub.\",\n",
    "        content_url=\"https://github.com/AllenInstitute/GRNN\",\n",
    "        encoding_format=\"git+https\",\n",
    "        sha256=\"main\",\n",
    "    ),\n",
    "    # Within that repository, a FileSet lists all JSONL files:\n",
    "    mlc.FileSet(\n",
    "        id=\"jsonl-files\",\n",
    "        name=\"jsonl-files\",\n",
    "        description=\"JSON files are hosted on the GitHub repository.\",\n",
    "        contained_in=[\"github-repository\"],\n",
    "        encoding_format=\"application/jsonlines\",\n",
    "        includes=\"model/*.jsonl\",\n",
    "    ),\n",
    "]\n",
    "record_sets = [\n",
    "    \n",
    "\n",
    "    # RecordSets contains records in the dataset.\n",
    "    mlc.RecordSet(\n",
    "        id=\"jsonl\",\n",
    "        name=\"jsonl\",\n",
    "        # Each record has one or many fields...\n",
    "        fields=[\n",
    "            # Fields can be extracted from the FileObjects/FileSets.\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/cell_id\",\n",
    "                name=\"cell_id\",\n",
    "                description=\"\",\n",
    "                data_types=mlc.DataType.INTEGER,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    # Extract the field from the column of a FileObject/FileSet:\n",
    "                    extract=mlc.Extract(column=\"cell_id\"),\n",
    "                ),\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/cre-line\",\n",
    "                name=\"cre-line\",\n",
    "                description=\"The expected completion of the promt.\",\n",
    "                data_types=mlc.DataType.TEXT,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    extract=mlc.Extract(column=\"cre-line\"),\n",
    "                ),\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/bin_size\",\n",
    "                name=\"bin_size\",\n",
    "                description=(\n",
    "                    \"The machine learning task appearing as the name of the\"\n",
    "                    \" file.\"\n",
    "                ),\n",
    "                data_types=mlc.DataType.INTEGER,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    extract=mlc.Extract(column=\"bin_size\"),\n",
    "                ),\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/actv_bin_size\",\n",
    "                name=\"actv_bin_size\",\n",
    "                description=(\n",
    "                    \"The machine learning task appearing as the name of the\"\n",
    "                    \" file.\"\n",
    "                ),\n",
    "                data_types=mlc.DataType.INTEGER,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    extract=mlc.Extract(column=\"actv_bin_size\"),\n",
    "                ),\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/val_evr\",\n",
    "                name=\"val_evr\",\n",
    "                description=(\n",
    "                    \"The machine learning task appearing as the name of the\"\n",
    "                    \" file.\"\n",
    "                ),\n",
    "                data_types=mlc.DataType.FLOAT,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    extract=mlc.Extract(column=\"val_evr\"),\n",
    "                ),\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/test_evr\",\n",
    "                name=\"test_evr\",\n",
    "                description=(\n",
    "                    \"The machine learning task appearing as the name of the\"\n",
    "                    \" file.\"\n",
    "                ),\n",
    "                data_types=mlc.DataType.FLOAT,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    extract=mlc.Extract(column=\"test_evr\"),\n",
    "                ),\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/train_loss\",\n",
    "                name=\"train_loss\",\n",
    "                description=(\n",
    "                    \"The machine learning task appearing as the name of the\"\n",
    "                    \" file.\"\n",
    "                ),\n",
    "                data_types=mlc.DataType.FLOAT,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    extract=mlc.Extract(column=\"train_loss\"),\n",
    "                ),\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/test_loss\",\n",
    "                name=\"test_loss\",\n",
    "                description=(\n",
    "                    \"The machine learning task appearing as the name of the\"\n",
    "                    \" file.\"\n",
    "                ),\n",
    "                data_types=mlc.DataType.FLOAT,\n",
    "                source=mlc.Source(\n",
    "                    file_set=\"jsonl-files\",\n",
    "                    extract=mlc.Extract(column=\"test_loss\"),\n",
    "                ),\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"jsonl/params\",\n",
    "                name=\"params\",\n",
    "                description=(\n",
    "                    \"The machine learning task appearing as the name of the\"\n",
    "                    \" file.\"\n",
    "                ),\n",
    "                # data_types=mlc.DataType.TEXT,\n",
    "                # source=mlc.Source(\n",
    "                #     file_set=\"jsonl-files\",\n",
    "                #     extract=mlc.Extract(column=\"params\"),\n",
    "                # ),\n",
    "                sub_fields = [\n",
    "                    mlc.Field(\n",
    "                        id=\"jsonl/params/a\",\n",
    "                        name=\"a\",\n",
    "                        description=\"The expected completion of the promt.\",\n",
    "                        data_types=mlc.DataType.FLOAT,\n",
    "                        repeated=True,\n",
    "                        source=mlc.Source(\n",
    "                            #field=\"id=jsonl/params\",\n",
    "                            file_set=\"jsonl-files\",\n",
    "                            \n",
    "                            extract=mlc.Extract(column=\"params\"),\n",
    "                            transforms=[mlc.Transform(json_path=\"a[0][0]\")], # MUSTFIX : this is not working\n",
    "                        ),\n",
    "                    ),\n",
    "                    mlc.Field(\n",
    "                        id=\"jsonl/params/b\",\n",
    "                        name=\"b\",\n",
    "                        description=\"The expected completion of the promt.\",\n",
    "                        data_types=mlc.DataType.FLOAT,\n",
    "                        repeated=True,\n",
    "                        source=mlc.Source(\n",
    "                            #field=\"id=jsonl/params\",\n",
    "                            file_set=\"jsonl-files\",\n",
    "                            \n",
    "                            extract=mlc.Extract(column=\"params\"),\n",
    "                            transforms=[mlc.Transform(json_path=\"b[0][0]\")], # MUSTFIX : this is not working\n",
    "                        ),\n",
    "                    ),\n",
    "                    mlc.Field(\n",
    "                        id=\"jsonl/params/ds\",\n",
    "                        name=\"ds\",\n",
    "                        description=\"The expected completion of the promt.\",\n",
    "                        data_types=mlc.DataType.FLOAT,\n",
    "                        repeated=True,\n",
    "                        source=mlc.Source(\n",
    "                            #field=\"id=jsonl/params\",\n",
    "                            file_set=\"jsonl-files\",\n",
    "                            \n",
    "                            extract=mlc.Extract(column=\"params\"),\n",
    "                            transforms=[mlc.Transform(json_path=\"ds[0]\")], # MUSTFIX : this is not working\n",
    "                        ),\n",
    "                    ),\n",
    "                    mlc.Field(\n",
    "                        id=\"jsonl/params/bin_size\",\n",
    "                        name=\"params_bin_size\",\n",
    "                        description=\"The expected completion of the promt.\",\n",
    "                        data_types=mlc.DataType.INTEGER,\n",
    "                        repeated=True,\n",
    "                        source=mlc.Source(\n",
    "                            #field=\"id=jsonl/params\",\n",
    "                            file_set=\"jsonl-files\",\n",
    "                            \n",
    "                            extract=mlc.Extract(column=\"params\"),\n",
    "                            transforms=[mlc.Transform(json_path=\"bin_size\")], # MUSTFIX : this is not working\n",
    "                        ),\n",
    "                    ),\n",
    "                    # mlc.Field(\n",
    "                    #     id=\"jsonl/params/g\",\n",
    "                    #     name=\"params_g\",\n",
    "                    #     description=\"The expected completion of the promt.\",\n",
    "                    #     data_types=mlc.DataType.FLOAT,\n",
    "                    #     # repeated=True,\n",
    "                    #     # source=mlc.Source(\n",
    "                    #     #     #field=\"id=jsonl/params\",\n",
    "                    #     #     file_set=\"jsonl-files\",\n",
    "                            \n",
    "                    #     #     extract=mlc.Extract(column=\"params\"),\n",
    "                    #     #     transforms=[mlc.Transform(json_path=\"a[0][0:4]\")],\n",
    "                    #     # ),\n",
    "                    #     sub_fields = [\n",
    "                    #         mlc.Field(\n",
    "                    #             id=\"jsonl/params/g/max_current\",\n",
    "                    #             name=\"max_current\",\n",
    "                    #             description=\"The expected completion of the promt.\",\n",
    "                    #             data_types=mlc.DataType.FLOAT,\n",
    "                    #             repeated=True,\n",
    "                    #             source=mlc.Source(\n",
    "                    #                 #field=\"id=jsonl/params\",\n",
    "                    #                 #file_set=\"jsonl-files\",\n",
    "                                    \n",
    "                    #                 extract=mlc.Extract(column=\"params\"),\n",
    "                    #                 #transforms=[mlc.Transform(json_path=\"params.g.max_current\")],\n",
    "                    #                 transforms=[mlc.Transform(json_path=\"bin_size\")],  # MUSTFIX : this is not working\n",
    "                    #             ),\n",
    "                    #         ),\n",
    "                    #         # mlc.Field(\n",
    "                    #         #     id=\"jsonl/params/g/max_firing_rate\",\n",
    "                    #         #     name=\"max_firing_rate\",\n",
    "                    #         #     description=\"The expected completion of the promt.\",\n",
    "                    #         #     data_types=mlc.DataType.FLOAT,\n",
    "                    #         #     repeated=True,\n",
    "                    #         #     source=mlc.Source(\n",
    "                    #         #         #field=\"id=jsonl/params\",\n",
    "                    #         #         file_set=\"jsonl-files\",\n",
    "                                    \n",
    "                    #         #         extract=mlc.Extract(column=\"params\"),\n",
    "                    #         #         transforms=[mlc.Transform(json_path=\"g.max_firing_rate\")],\n",
    "                    #         #     ),\n",
    "                    #         # ),\n",
    "                    #         # mlc.Field(\n",
    "                    #         #     id=\"jsonl/params/g/poly_coeff\",\n",
    "                    #         #     name=\"poly_coeff\",\n",
    "                    #         #     description=\"The expected completion of the promt.\",\n",
    "                    #         #     data_types=mlc.DataType.FLOAT,\n",
    "                    #         #     repeated=True,\n",
    "                    #         #     source=mlc.Source(\n",
    "                    #         #         #field=\"id=jsonl/params\",\n",
    "                    #         #         file_set=\"jsonl-files\",\n",
    "                                    \n",
    "                    #         #         extract=mlc.Extract(column=\"params\"),\n",
    "                    #         #         transforms=[mlc.Transform(json_path=\"g.poly_coeff\")],\n",
    "                    #         #     ),\n",
    "                    #         # ),\n",
    "                    #         # mlc.Field(\n",
    "                    #         #     id=\"jsonl/params/g/b\",\n",
    "                    #         #     name=\"g_b\",\n",
    "                    #         #     description=\"The expected completion of the promt.\",\n",
    "                    #         #     data_types=mlc.DataType.FLOAT,\n",
    "                    #         #     repeated=True,\n",
    "                    #         #     source=mlc.Source(\n",
    "                    #         #         #field=\"id=jsonl/params\",\n",
    "                    #         #         file_set=\"jsonl-files\",\n",
    "                                    \n",
    "                    #         #         extract=mlc.Extract(column=\"params\"),\n",
    "                    #         #         transforms=[mlc.Transform(json_path=\"g.b\")],\n",
    "                    #         #     ),\n",
    "                    #         # ),\n",
    "                    #         # mlc.Field(\n",
    "                    #         #     id=\"jsonl/params/g/bin_size\",\n",
    "                    #         #     name=\"g_bin_size\",\n",
    "                    #         #     description=\"The expected completion of the promt.\",\n",
    "                    #         #     data_types=mlc.DataType.INTEGER,\n",
    "                    #         #     repeated=True,\n",
    "                    #         #     source=mlc.Source(\n",
    "                    #         #         #field=\"id=jsonl/params\",\n",
    "                    #         #         file_set=\"jsonl-files\",\n",
    "                                    \n",
    "                    #         #         extract=mlc.Extract(column=\"params\"),\n",
    "                    #         #         transforms=[mlc.Transform(json_path=\"g.bin_size\")],\n",
    "                    #         #     ),\n",
    "                    #         # ),\n",
    "                    #     ]\n",
    "                    # ),\n",
    "                ],\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "# Metadata contains information about the dataset.\n",
    "metadata = mlc.Metadata(\n",
    "    name=\"gpt-3\",\n",
    "    # Descriptions can contain plain text or markdown.\n",
    "    description=(\n",
    "        \"Recent work has demonstrated substantial gains on many NLP tasks and\"\n",
    "        \" benchmarks by pre-training on a large corpus of text followed by\"\n",
    "        \" fine-tuning on a specific task. While typically task-agnostic in\"\n",
    "        \" architecture, this method still requires task-specific fine-tuning\"\n",
    "        \" datasets of thousands or tens of thousands of examples. By contrast,\"\n",
    "        \" humans can generally perform a new language task from only a few\"\n",
    "        \" examples or from simple instructions \\u2013 something which current\"\n",
    "        \" NLP systems still largely struggle to do. Here we show that scaling\"\n",
    "        \" up language models greatly improves task-agnostic, few-shot\"\n",
    "        \" performance, sometimes even reaching competitiveness with prior\"\n",
    "        \" state-of-the-art fine-tuning approaches. Specifically, we train\"\n",
    "        \" GPT-3, an autoregressive language model with 175 billion parameters,\"\n",
    "        \" 10x more than any previous non-sparse language model, and test its\"\n",
    "        \" performance in the few-shot setting. For all tasks, GPT-3 is applied\"\n",
    "        \" without any gradient updates or fine-tuning, with tasks and few-shot\"\n",
    "        \" demonstrations specified purely via text interaction with the model.\"\n",
    "        \" GPT-3 achieves strong performance on many NLP datasets, including\"\n",
    "        \" translation, question-answering, and cloze tasks, as well as several\"\n",
    "        \" tasks that require on-the-fly reasoning or domain adaptation, such as\"\n",
    "        \" unscrambling words, using a novel word in a sentence, or performing\"\n",
    "        \" 3-digit arithmetic. At the same time, we also identify some datasets\"\n",
    "        \" where GPT-3's few-shot learning still struggles, as well as some\"\n",
    "        \" datasets where GPT-3 faces methodological issues related to training\"\n",
    "        \" on large web corpora. Finally, we find that GPT-3 can generate\"\n",
    "        \" samples of news articles which human evaluators have difficulty\"\n",
    "        \" distinguishing from articles written by humans. We discuss broader\"\n",
    "        \" societal impacts of this finding and of GPT-3 in general.\"\n",
    "    ),\n",
    "    cite_as=(\n",
    "        \"@article{brown2020language, title={Language Models are Few-Shot\"\n",
    "        \" Learners}, author={Tom B. Brown and Benjamin Mann and Nick Ryder and\"\n",
    "        \" Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind\"\n",
    "        \" Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and\"\n",
    "        \" Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom\"\n",
    "        \" Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and\"\n",
    "        \" Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and\"\n",
    "        \" Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and\"\n",
    "        \" Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford\"\n",
    "        \" and Ilya Sutskever and Dario Amodei}, year={2020},\"\n",
    "        \" eprint={2005.14165}, archivePrefix={arXiv}, primaryClass={cs.CL} }\"\n",
    "    ),\n",
    "    url=\"https://github.com/AllenInstitute/GRNN\",\n",
    "    distribution=distribution,\n",
    "    record_sets=record_sets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(metadata.issues.report())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"croissant.json\", \"w\") as f:\n",
    "  content = metadata.to_json()\n",
    "  content = json.dumps(content, indent=2)\n",
    "  #print(content)\n",
    "  f.write(content)\n",
    "  f.write(\"\\n\")  # Terminate file with newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "GenerationError",
     "evalue": "An error occured during the streaming generation of the dataset, more specifically during the operation Read(jsonl-files)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlcroissant/_src/operation_graph/execute.py:97\u001b[0m, in \u001b[0;36mexecute_operations_in_streaming\u001b[0;34m(record_set, operations, list_of_operations, result)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m operation(result)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlcroissant/_src/operation_graph/operations/field.py:165\u001b[0m, in \u001b[0;36mReadFields.__call__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[0;32m--> 165\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_result\u001b[39m(row):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlcroissant/_src/operation_graph/operations/field.py:122\u001b[0m, in \u001b[0;36m_extract_value\u001b[0;34m(df, field)\u001b[0m\n\u001b[1;32m    121\u001b[0m source \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39msource\n\u001b[0;32m--> 122\u001b[0m column_name \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m df:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlcroissant/_src/structure_graph/nodes/source.py:242\u001b[0m, in \u001b[0;36mSource.get_column\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muuid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo UUID! This case already rose an issue and should not happen at run\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m time.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m     )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract\u001b[38;5;241m.\u001b[39mcolumn:\n",
      "\u001b[0;31mValueError\u001b[0m: No UUID! This case already rose an issue and should not happen at run time.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGenerationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlcroissant/_src/operation_graph/execute.py:115\u001b[0m, in \u001b[0;36mexecute_operations_in_streaming\u001b[0;34m(record_set, operations, list_of_operations, result)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m execute_operations_in_streaming(\n\u001b[1;32m    109\u001b[0m             record_set\u001b[38;5;241m=\u001b[39mrecord_set,\n\u001b[1;32m    110\u001b[0m             operations\u001b[38;5;241m=\u001b[39moperations,\n\u001b[1;32m    111\u001b[0m             list_of_operations\u001b[38;5;241m=\u001b[39mlist_of_operations[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :],\n\u001b[1;32m    112\u001b[0m             result\u001b[38;5;241m=\u001b[39moperation(file),\n\u001b[1;32m    113\u001b[0m         )\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m read_all_files()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlcroissant/_src/operation_graph/execute.py:108\u001b[0m, in \u001b[0;36mexecute_operations_in_streaming.<locals>.read_all_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, operation)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m execute_operations_in_streaming(\n\u001b[1;32m    109\u001b[0m     record_set\u001b[38;5;241m=\u001b[39mrecord_set,\n\u001b[1;32m    110\u001b[0m     operations\u001b[38;5;241m=\u001b[39moperations,\n\u001b[1;32m    111\u001b[0m     list_of_operations\u001b[38;5;241m=\u001b[39mlist_of_operations[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :],\n\u001b[1;32m    112\u001b[0m     result\u001b[38;5;241m=\u001b[39moperation(file),\n\u001b[1;32m    113\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlcroissant/_src/operation_graph/execute.py:123\u001b[0m, in \u001b[0;36mexecute_operations_in_streaming\u001b[0;34m(record_set, operations, list_of_operations, result)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GenerationError(\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occured during the streaming generation of the dataset, more\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m specifically during the operation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mGenerationError\u001b[0m: An error occured during the streaming generation of the dataset, more specifically during the operation ReadFields(jsonl)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGenerationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m mlc\u001b[38;5;241m.\u001b[39mDataset(jsonld\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcroissant.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m records \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mrecords(record_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, record \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(records):\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(record)\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlcroissant/_src/datasets.py:135\u001b[0m, in \u001b[0;36mRecords.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m can_stream_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, d \u001b[38;5;129;01min\u001b[39;00m operations\u001b[38;5;241m.\u001b[39mdegree())\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_stream_dataset:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m execute_operations_in_streaming(\n\u001b[1;32m    136\u001b[0m         record_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_set,\n\u001b[1;32m    137\u001b[0m         operations\u001b[38;5;241m=\u001b[39moperations,\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m execute_operations_sequentially(\n\u001b[1;32m    141\u001b[0m         record_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_set, operations\u001b[38;5;241m=\u001b[39moperations\n\u001b[1;32m    142\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlcroissant/_src/operation_graph/execute.py:123\u001b[0m, in \u001b[0;36mexecute_operations_in_streaming\u001b[0;34m(record_set, operations, list_of_operations, result)\u001b[0m\n\u001b[1;32m    121\u001b[0m         result \u001b[38;5;241m=\u001b[39m operation(result)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GenerationError(\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occured during the streaming generation of the dataset, more\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m specifically during the operation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mGenerationError\u001b[0m: An error occured during the streaming generation of the dataset, more specifically during the operation Read(jsonl-files)"
     ]
    }
   ],
   "source": [
    "dataset = mlc.Dataset(jsonld=\"croissant.json\")\n",
    "records = dataset.records(record_set=\"jsonl\")\n",
    "\n",
    "for i, record in enumerate(records):\n",
    "  print(record)\n",
    "  if i > 10:\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
