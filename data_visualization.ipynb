{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5275b-b4b8-45fd-a52b-8c4465b6ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "from model import GFR\n",
    "from evaluate import explained_variance_ratio\n",
    "from data import get_data, get_train_test_data, preprocess_data\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec2c08-48c8-476f-9ed1-ca756f3f889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(bin_size, activation_bin_size, C):\n",
    "    params = {}\n",
    "    save_path = f\"model/params/{bin_size}_{activation_bin_size}_{C}/\"\n",
    "    for fname in os.listdir(save_path):\n",
    "        if fname.endswith(\".pickle\"):\n",
    "            cell_id = int(fname.split(\".\")[0])\n",
    "            with open(f\"{save_path}{fname}\", \"rb\") as f:\n",
    "                params[cell_id] = pickle.load(f)\n",
    "    return params\n",
    "\n",
    "def get_all_params():\n",
    "    bin_sizes = [10, 20, 50, 100]\n",
    "    activation_bin_sizes = [20, 100]\n",
    "    C = [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0]\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    for bin_size in bin_sizes:\n",
    "        for activation_bin_size in activation_bin_sizes:\n",
    "            if activation_bin_size >= bin_size:\n",
    "                for c in C:\n",
    "                    params[(bin_size, activation_bin_size, c)] = get_params(bin_size, activation_bin_size, c)\n",
    "                                \n",
    "    return params\n",
    "\n",
    "# summarize params of one configuration\n",
    "def summarize(params):\n",
    "    data = {\"cell_id\": [], \"evr1\": [], \"evr2\": [], \"loss\": [], \"epochs\": []}\n",
    "\n",
    "    for cell_id in params:\n",
    "        data[\"cell_id\"].append(cell_id)\n",
    "        data[\"evr1\"].append(params[cell_id][\"evr1\"])\n",
    "        data[\"evr2\"].append(params[cell_id][\"evr2\"])\n",
    "        data[\"loss\"].append(params[cell_id][\"train_losses\"][-1])\n",
    "        data[\"epochs\"].append(len(params[cell_id][\"train_losses\"]))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index(\"cell_id\")\n",
    "    df = df.sort_values(\"evr2\")\n",
    "    df_corrected = df[df[\"evr1\"] > 0.01].dropna()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return {}\n",
    "    \n",
    "    return {\n",
    "        \"n_cells\": len(df),\n",
    "        \"p_zero_evr\": len(df[df['evr2'] < 0.01]) / len(df),\n",
    "        \"p_early_stop\": len(df[df['epochs'] < 50]) / len(df),\n",
    "        \"corrected_median_evr\": np.median(df_corrected['evr2'].values)\n",
    "    }\n",
    "\n",
    "def get_best_params_for_actv_bin_size(params, bin_size, actv_bin_size):\n",
    "    best_params = {}\n",
    "    \n",
    "    cell_ids = set()\n",
    "    for config in params:\n",
    "        cell_ids = cell_ids.union(set(params[config].keys()))\n",
    "    \n",
    "    for cell_id in cell_ids:\n",
    "        best_config = None\n",
    "        best_evr = -1e10\n",
    "        \n",
    "        for config in params:\n",
    "            if config[0] == bin_size and config[1] == actv_bin_size and cell_id in params[config] and params[config][cell_id][\"evr1\"] > best_evr:\n",
    "                best_evr = params[config][cell_id][\"evr1\"]\n",
    "                best_config = config\n",
    "        \n",
    "        # doesn't make sense\n",
    "        if best_config is not None:\n",
    "            best_params[cell_id] = params[best_config][cell_id]\n",
    "        \n",
    "    return best_params\n",
    "\n",
    "def get_best_params(params, bin_size):\n",
    "    best_params = {}\n",
    "    \n",
    "    cell_ids = set()\n",
    "    for config in params:\n",
    "        cell_ids = cell_ids.union(set(params[config].keys()))\n",
    "    \n",
    "    for cell_id in cell_ids:\n",
    "        best_config = None\n",
    "        best_evr = -1e10\n",
    "        \n",
    "        for config in params:\n",
    "            if config[0] == bin_size and cell_id in params[config] and params[config][cell_id][\"evr1\"] > best_evr:\n",
    "                best_evr = params[config][cell_id][\"evr1\"]\n",
    "                best_config = config\n",
    "        \n",
    "        # doesn't make sense\n",
    "        if best_config is not None:\n",
    "            best_params[cell_id] = params[best_config][cell_id]\n",
    "        \n",
    "    return best_params\n",
    "\n",
    "# summarize params of one configuration\n",
    "def summarize(params):\n",
    "    data = {\"cell_id\": [], \"evr1\": [], \"evr2\": [], \"loss\": [], \"epochs\": []}\n",
    "\n",
    "    for cell_id in params:\n",
    "        data[\"cell_id\"].append(cell_id)\n",
    "        data[\"evr1\"].append(params[cell_id][\"evr1\"])\n",
    "        data[\"evr2\"].append(params[cell_id][\"evr2\"])\n",
    "        data[\"loss\"].append(params[cell_id][\"train_losses\"][-1])\n",
    "        data[\"epochs\"].append(len(params[cell_id][\"train_losses\"]))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index(\"cell_id\")\n",
    "    df = df.sort_values(\"evr2\")\n",
    "    df_corrected = df[df[\"evr1\"] > 0.01].dropna()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return {}\n",
    "    \n",
    "    return {\n",
    "        \"n_cells\": len(df),\n",
    "        \"p_zero_evr\": len(df[df['evr2'] < 0.01]) / len(df),\n",
    "        \"p_early_stop\": len(df[df['epochs'] < 50]) / len(df),\n",
    "        \"corrected_median_evr\": np.median(df_corrected['evr2'].values)\n",
    "    }\n",
    "\n",
    "def visualize_data(params):\n",
    "    data = {\"cell_id\": [], \"evr1\": [], \"evr2\": [], \"loss\": [], \"epochs\": []}\n",
    "\n",
    "    for cell_id in params:\n",
    "        data[\"cell_id\"].append(cell_id)\n",
    "        data[\"evr1\"].append(params[cell_id][\"evr1\"])\n",
    "        data[\"evr2\"].append(params[cell_id][\"evr2\"])\n",
    "        data[\"loss\"].append(params[cell_id][\"train_losses\"][-1])\n",
    "        data[\"epochs\"].append(len(params[cell_id][\"train_losses\"]))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index(\"cell_id\")\n",
    "    df = df.sort_values(\"evr2\")\n",
    "\n",
    "    print(f\"Total number of cells: {len(df)}\")\n",
    "    print(f\"Number/proportion of cells with evr<=0: {len(df[df['evr2'] <= 0])}/{len(df[df['evr2'] <= 0]) / len(df)}\")\n",
    "    print(f\"Number/proportion of cells with epochs<50: {len(df[df['epochs'] < 50])}/{len(df[df['epochs'] < 50]) / len(df)}\")\n",
    "\n",
    "    df_corrected = df[df[\"evr1\"] > 0.01]\n",
    "    print(f\"Median evr: {np.median(df_corrected.dropna()['evr2'].values)}\")\n",
    "\n",
    "    evrs1 = df_corrected.iloc[:, 0]\n",
    "    evrs2 = df_corrected.iloc[:, 1]\n",
    "    losses = df_corrected.iloc[:, 2]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(evrs2, bins=\"auto\")\n",
    "    plt.xlabel(\"evr2\")\n",
    "    plt.ylabel(\"counts\")\n",
    "    plt.title(\"evr2 histogram (failed optimizations removed)\")\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(losses, bins=\"auto\")\n",
    "    plt.xlabel(\"loss\")\n",
    "    plt.ylabel(\"counts\")\n",
    "    plt.title(\"loss histogram (failed optimizations removed)\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(evrs2, losses, alpha=0.5)\n",
    "    plt.xlabel(\"evr2\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"evr2 vs loss scatter plot (failed optimizations removed)\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(evrs1, evrs2, alpha=0.5)\n",
    "    plt.xlabel(\"evr1\")\n",
    "    plt.ylabel(\"evr2\")\n",
    "    plt.title(\"evr1 vs evr2 scatter plot (failed optimizations removed)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_line_name(df, cell_id):\n",
    "    return df[df[\"specimen__id\"] == cell_id][\"line_name\"].to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc958144",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/best_params.pickle\", \"rb\") as f:\n",
    "    params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166827cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "evrs = {}\n",
    "medians = []\n",
    "bin_sizes = [10, 20, 50, 100]\n",
    "ps = []\n",
    "for bin_size in bin_sizes:\n",
    "    ps.append(get_best_params(params, bin_size))\n",
    "    \n",
    "for i, p in enumerate(ps):\n",
    "    evr = []\n",
    "    for cell_id in p:\n",
    "        evr.append(p[cell_id][\"evr2\"])\n",
    "    medians.append(np.median(evr))\n",
    "    evrs[f\"$\\\\Delta t={bin_sizes[i]}$\"] = evr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3e173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3.2), dpi=1000)\n",
    "plt.hist(evrs.values(), histtype='step', bins=\"auto\", label=[f\"$\\\\Delta t={x}$\" for x in bin_sizes])\n",
    "plt.xlabel(\"explained variance ratio\");\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6748c27-3ed9-417f-aaef-d159b5070548",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 20\n",
    "actv_bin_size = 20\n",
    "df = visualize_data(params[(bin_size, actv_bin_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc29d2-4c00-401e-93c1-6fc1d938b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(cell_id, params, bin_size=20):\n",
    "    model = GFR.from_params(params[cell_id][\"params\"])\n",
    "    data = get_data(cell_id)\n",
    "    Is_tr, fs_tr, _, _, Is_te, fs_te, stims = get_train_test_data(data, bin_size)\n",
    "    Is_actv, fs_actv = preprocess_data(data, bin_size=params[cell_id][\"params\"][\"g\"][\"bin_size\"])\n",
    "    \n",
    "    def _plot_train_data():\n",
    "        for Is, fs, s in zip(Is_tr, fs_tr, stims):\n",
    "            if torch.sum(fs) > 0.2:\n",
    "                for i in range(Is.shape[0]):\n",
    "                    utils.plot_predictions(\n",
    "                        model, \n",
    "                        Is[i, :], \n",
    "                        fs[i, :],\n",
    "                        bin_size\n",
    "                    )\n",
    "                \n",
    "    def _plot_noise2():\n",
    "        r = explained_variance_ratio(model, Is_te[0], fs_te[0], bin_size)\n",
    "        print(Is_te[0][0, :].shape)\n",
    "        utils.plot_predictions(\n",
    "            model, \n",
    "            Is_te[0][0, :], \n",
    "            fs_te[0][0, :], \n",
    "            bin_size, \n",
    "            xlim = [15, 20]\n",
    "        )\n",
    "    \n",
    "    def _plot_kernel():\n",
    "        utils.plot_kernel(\n",
    "            model,\n",
    "            cell_id,\n",
    "            bin_size,\n",
    "            save = False,\n",
    "            xlim = 25\n",
    "        )\n",
    "    \n",
    "    def _plot_activation():\n",
    "        utils.plot_activation(\n",
    "            Is_actv,\n",
    "            fs_actv,\n",
    "            model.g\n",
    "        )\n",
    "    \n",
    "    def _plot_losses():\n",
    "        train_losses = params[cell_id][\"train_losses\"]\n",
    "        test_losses = params[cell_id][\"test_losses\"]\n",
    "        plt.figure()\n",
    "        plt.plot(list(range(len(train_losses))), train_losses, label=\"Train\")\n",
    "        plt.plot(list(range(len(test_losses))), test_losses, label=\"Test\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Poisson NLL Loss\")\n",
    "        plt.legend()\n",
    "        \n",
    "    _plot_train_data()\n",
    "    _plot_noise2()\n",
    "    _plot_kernel()\n",
    "    _plot_activation()\n",
    "    _plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052360e7-9da8-4166-9137-81bb7adf0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_id = 475202388\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "visualize(cell_id, params[(bin_size, actv_bin_size)], bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052360e7-9da8-4166-9137-81bb7adf0f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
