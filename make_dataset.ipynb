{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c388ba-3100-43e6-b25d-9d8997d7197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from model import GFR, PolynomialActivation\n",
    "from evaluate import explained_variance_ratio\n",
    "from data import get_data, get_train_test_data, preprocess_data\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2bd7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(params, threshold=0.6):\n",
    "    with open(\"model/labels.pickle\", \"rb\") as f:\n",
    "        labels = pickle.load(f)\n",
    "    \n",
    "    chosen_ids = filter(lambda x: params[x][\"evr2\"] >= threshold, params.keys())\n",
    "    \n",
    "    dataset = {}\n",
    "    for cell_id in chosen_ids:\n",
    "        y = labels[cell_id]\n",
    "        p = params[cell_id][\"params\"]\n",
    "        model = GFR.from_params(p)\n",
    "        \n",
    "        a = p[\"a\"].reshape(-1)\n",
    "        b = p[\"b\"].reshape(-1)\n",
    "        pc = p[\"g\"][\"poly_coeff\"].reshape(-1)\n",
    "        gb = p[\"g\"][\"b\"].reshape(-1)\n",
    "        mc = p[\"g\"][\"max_current\"].reshape(-1)\n",
    "        mfr = p[\"g\"][\"max_firing_rate\"].reshape(-1)\n",
    "        x = torch.cat([a, b, pc, gb, mc, mfr])\n",
    "        \n",
    "        dataset[cell_id] = (x, y, params[cell_id][\"evr2\"])\n",
    "        \n",
    "    return dataset\n",
    "    \n",
    "def get_params(bin_size, activation_bin_size, C, patch_seq=False):\n",
    "    params = {}\n",
    "    save_path = f\"model/params/{bin_size}_{activation_bin_size}_{C}/\"\n",
    "    if patch_seq:\n",
    "        save_path = f\"model/params/patch_seq_{bin_size}_{activation_bin_size}_{C}/\"\n",
    "    for fname in os.listdir(save_path):\n",
    "        if fname.endswith(\".pickle\"):\n",
    "            cell_id = int(fname.split(\".\")[0])\n",
    "            with open(f\"{save_path}{fname}\", \"rb\") as f:\n",
    "                params[cell_id] = pickle.load(f)\n",
    "    return params\n",
    "\n",
    "def get_all_params(patch_seq=False):\n",
    "    bin_sizes = [10, 20, 50, 100]\n",
    "    activation_bin_sizes = [20, 100]\n",
    "    C = [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0]\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    for bin_size in bin_sizes:\n",
    "        for activation_bin_size in activation_bin_sizes:\n",
    "            if activation_bin_size >= bin_size:\n",
    "                for c in C:\n",
    "                    params[(bin_size, activation_bin_size, c)] = get_params(bin_size, activation_bin_size, c, patch_seq=patch_seq)\n",
    "                                \n",
    "    return params\n",
    "\n",
    "# summarize params of one configuration\n",
    "def summarize(params):\n",
    "    data = {\"cell_id\": [], \"evr1\": [], \"evr2\": [], \"loss\": [], \"epochs\": []}\n",
    "\n",
    "    for cell_id in params:\n",
    "        data[\"cell_id\"].append(cell_id)\n",
    "        data[\"evr1\"].append(params[cell_id][\"evr1\"])\n",
    "        data[\"evr2\"].append(params[cell_id][\"evr2\"])\n",
    "        data[\"loss\"].append(params[cell_id][\"train_losses\"][-1])\n",
    "        data[\"epochs\"].append(len(params[cell_id][\"train_losses\"]))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index(\"cell_id\")\n",
    "    df = df.sort_values(\"evr2\")\n",
    "    df_corrected = df[df[\"evr1\"] > 0.01].dropna()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return {}\n",
    "    \n",
    "    return {\n",
    "        \"n_cells\": len(df),\n",
    "        \"p_zero_evr\": len(df[df['evr2'] < 0.01]) / len(df),\n",
    "        \"p_early_stop\": len(df[df['epochs'] < 50]) / len(df),\n",
    "        \"median_evr\": np.median(df_corrected['evr2'].values)\n",
    "    }\n",
    "\n",
    "def get_best_params_for_actv_bin_size(params, bin_size, actv_bin_size):\n",
    "    best_params = {}\n",
    "    \n",
    "    cell_ids = set()\n",
    "    for config in params:\n",
    "        cell_ids = cell_ids.union(set(params[config].keys()))\n",
    "    \n",
    "    for cell_id in cell_ids:\n",
    "        best_config = None\n",
    "        best_evr = -1e10\n",
    "        \n",
    "        for config in params:\n",
    "            if config[0] == bin_size and config[1] == actv_bin_size and cell_id in params[config] and params[config][cell_id][\"evr1\"] > best_evr:\n",
    "                best_evr = params[config][cell_id][\"evr1\"]\n",
    "                best_config = config\n",
    "        \n",
    "        # deals with NaN values\n",
    "        if best_config is not None:\n",
    "            best_params[cell_id] = params[best_config][cell_id]\n",
    "        \n",
    "    return best_params\n",
    "    \n",
    "def get_best_params(params, bin_size):\n",
    "    best_params = {}\n",
    "    \n",
    "    cell_ids = set()\n",
    "    for config in params:\n",
    "        cell_ids = cell_ids.union(set(params[config].keys()))\n",
    "    \n",
    "    for cell_id in cell_ids:\n",
    "        best_config = None\n",
    "        best_evr = -1e10\n",
    "        \n",
    "        for config in params:\n",
    "            if config[0] == bin_size and cell_id in params[config] and params[config][cell_id][\"evr1\"] > best_evr:\n",
    "                best_evr = params[config][cell_id][\"evr1\"]\n",
    "                best_config = config\n",
    "        \n",
    "        # doesn't make sense\n",
    "        if best_config is not None:\n",
    "            best_params[cell_id] = params[best_config][cell_id]\n",
    "        \n",
    "    return best_params\n",
    "\n",
    "def visualize_data(params):\n",
    "    data = {\"cell_id\": [], \"evr1\": [], \"evr2\": [], \"loss\": [], \"epochs\": []}\n",
    "\n",
    "    for cell_id in params:\n",
    "        data[\"cell_id\"].append(cell_id)\n",
    "        data[\"evr1\"].append(params[cell_id][\"evr1\"])\n",
    "        data[\"evr2\"].append(params[cell_id][\"evr2\"])\n",
    "        data[\"loss\"].append(params[cell_id][\"train_losses\"][-1])\n",
    "        data[\"epochs\"].append(len(params[cell_id][\"train_losses\"]))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index(\"cell_id\")\n",
    "    df = df.sort_values(\"evr2\")\n",
    "\n",
    "    print(f\"Total number of cells: {len(df)}\")\n",
    "    print(f\"Number/proportion of cells with evr<=0: {len(df[df['evr2'] <= 0])}/{len(df[df['evr2'] <= 0]) / len(df)}\")\n",
    "    print(f\"Number/proportion of cells with epochs<50: {len(df[df['epochs'] < 50])}/{len(df[df['epochs'] < 50]) / len(df)}\")\n",
    "\n",
    "    df_corrected = df[df[\"evr2\"].notna()]\n",
    "    print(f\"Median evr: {np.median(df_corrected.dropna()['evr2'].values)}\")\n",
    "\n",
    "    evrs1 = df_corrected.iloc[:, 0]\n",
    "    evrs2 = df_corrected.iloc[:, 1]\n",
    "    losses = df_corrected.iloc[:, 2]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(evrs2, bins=\"auto\")\n",
    "    plt.xlabel(\"evr2\")\n",
    "    plt.ylabel(\"counts\")\n",
    "    plt.title(\"evr2 histogram (failed optimizations removed)\")\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(losses, bins=\"auto\")\n",
    "    plt.xlabel(\"loss\")\n",
    "    plt.ylabel(\"counts\")\n",
    "    plt.title(\"loss histogram (failed optimizations removed)\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(evrs2, losses, alpha=0.5)\n",
    "    plt.xlabel(\"evr2\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"evr2 vs loss scatter plot (failed optimizations removed)\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(evrs1, evrs2, alpha=0.5)\n",
    "    plt.xlabel(\"evr1\")\n",
    "    plt.ylabel(\"evr2\")\n",
    "    plt.title(\"evr1 vs evr2 scatter plot (failed optimizations removed)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def summarize_all_models(all_params):\n",
    "    summ = {}\n",
    "    for config in all_params:\n",
    "        summ[f\"bin_size={config[0]}, activation_bin_size={config[1]}, C={config[2]}\"] = summarize(all_params[config])\n",
    "    for bin_size in [10, 20, 50, 100]:\n",
    "        for actv_bin_size in [20, 100]:\n",
    "            if bin_size <= actv_bin_size:\n",
    "                best_params = get_best_params_for_actv_bin_size(all_params, bin_size, actv_bin_size)\n",
    "                summ[f\"Best for {bin_size=}, {actv_bin_size=}:\"] = summarize(best_params)\n",
    "    for bin_size in [10, 20, 50, 100]:\n",
    "        best_params = get_best_params(all_params, bin_size)\n",
    "        summ[f\"Best for {bin_size=}:\"] = summarize(best_params)\n",
    "    print(nice_dict(summ))\n",
    "    \n",
    "def nice_dict(d, indent=0):\n",
    "    s = []\n",
    "    for i in d:\n",
    "        if type(d[i]) == dict:\n",
    "            s.append(f\"{i}\\n{nice_dict(d[i], indent=indent+1)}\")\n",
    "        elif type(d[i]) == float:\n",
    "            s.append(f\"{i}: {d[i]:.4f}\")\n",
    "        else:\n",
    "            s.append(f\"{i}: {d[i]}\")\n",
    "    return \"\\n\".join([\"| \"*indent + x for x in s])\n",
    "\n",
    "def save_best_params(all_params):\n",
    "    best_params = {}\n",
    "    for bin_size in [10, 20, 50, 100]:\n",
    "        for actv_bin_size in [20, 100]:\n",
    "            if bin_size <= actv_bin_size:\n",
    "                best_params[(bin_size, actv_bin_size)] = get_best_params_for_actv_bin_size(all_params, bin_size, actv_bin_size)\n",
    "    with open(\"model/best_params.pickle\", \"wb\") as f:\n",
    "        pickle.dump(best_params, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d2dfbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/metadata.csv\")\n",
    "\n",
    "def get_line_name(df, cell_id):\n",
    "    return df[df[\"specimen__id\"] == cell_id][\"line_name\"].to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee80c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/best_params_99_full.pickle\", \"rb\") as f:\n",
    "    all_params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b123ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 20\n",
    "actv_bin_size = 100\n",
    "params = all_params[(bin_size, actv_bin_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67698abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell_id in params:\n",
    "    p = params[cell_id][\"params\"]\n",
    "    cell_type = get_line_name(df, cell_id)\n",
    "    train_evr = params[cell_id][\"evr1\"]\n",
    "    val_evr = params[cell_id][\"evr2\"]\n",
    "    train_mse = params[cell_id][\"train_losses\"][-1]\n",
    "    test_mse = params[cell_id][\"test_losses\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a5fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_id = list(params.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080579da",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = params[cell_id][\"params\"]\n",
    "model = GFR.from_params(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1974df4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'a': tensor([[ 8.0004e+00,  1.3346e-03,  1.0409e-03,  7.7979e-04, -2.9032e-05,\n",
       "           -2.8466e-03, -6.2044e-03, -9.9217e-03]]),\n",
       "  'b': tensor([[-0.0322, -0.0330, -0.0339, -0.0345, -0.0348, -0.0343, -0.0335, -0.0328]]),\n",
       "  'g': {'max_current': Parameter containing:\n",
       "   tensor([169.9915]),\n",
       "   'max_firing_rate': Parameter containing:\n",
       "   tensor([0.0815], dtype=torch.float64),\n",
       "   'poly_coeff': tensor([[-2.8457e-01,  9.0654e-01,  4.1583e-23,  1.0626e-15]]),\n",
       "   'b': tensor([48.4476]),\n",
       "   'bin_size': 100},\n",
       "  'ds': tensor([1.0000, 0.6321, 0.3297, 0.1813, 0.0952, 0.0392, 0.0198, 0.0100]),\n",
       "  'bin_size': 20},\n",
       " 'evr1': 0.2126748905506103,\n",
       " 'evr2': 0.30039277317421137,\n",
       " 'train_losses': [0.12678318073164233,\n",
       "  0.1267899627169833,\n",
       "  0.12680980089374244,\n",
       "  0.12683317489626902,\n",
       "  0.12685919281129168,\n",
       "  0.12688775898759214,\n",
       "  0.1269187919423955,\n",
       "  0.1269525843526269,\n",
       "  0.1269891026464727,\n",
       "  0.12702822716609485,\n",
       "  0.1270700218981745,\n",
       "  0.12711512791453647,\n",
       "  0.12716348770805402,\n",
       "  0.127214734770462,\n",
       "  0.12726861599434233,\n",
       "  0.12732531061255678,\n",
       "  0.1273846588798855,\n",
       "  0.1274467355035177,\n",
       "  0.12751155566243644,\n",
       "  0.1275791596982789,\n",
       "  0.12764949288776437,\n",
       "  0.12772266504191132,\n",
       "  0.12779950047299193,\n",
       "  0.1278789515776386,\n",
       "  0.12796084177862738,\n",
       "  0.12804517721410452,\n",
       "  0.1281317371513759,\n",
       "  0.12822047111387633,\n",
       "  0.12831101055641528,\n",
       "  0.12840317073822166,\n",
       "  0.12849688686951244,\n",
       "  0.128591596349066,\n",
       "  0.128687069434323,\n",
       "  0.12878281412138678,\n",
       "  0.12887911169034047,\n",
       "  0.12897537674699852,\n",
       "  0.12907072853796261,\n",
       "  0.12916488623043762,\n",
       "  0.12925758620177477,\n",
       "  0.12934795021376064,\n",
       "  0.12943572118596447,\n",
       "  0.12952047904215436,\n",
       "  0.12960206214856093,\n",
       "  0.1296801869216757,\n",
       "  0.1297545662976424,\n",
       "  0.12982473581787377,\n",
       "  0.12989046059563766,\n",
       "  0.1299514862875317,\n",
       "  0.1300079323072222,\n",
       "  0.13005968759618206,\n",
       "  0.13010669705586952,\n",
       "  0.13014908513032095,\n",
       "  0.1301870460830054,\n",
       "  0.1302205636660582,\n",
       "  0.130249982380596,\n",
       "  0.13027544029457397,\n",
       "  0.13029664299017282,\n",
       "  0.13031388538219854,\n",
       "  0.13032758639949193,\n",
       "  0.130338083150312,\n",
       "  0.13034590373059593,\n",
       "  0.1303509378942819,\n",
       "  0.13035356118398952,\n",
       "  0.13035416372061578,\n",
       "  0.1303531802566267,\n",
       "  0.130350792422665,\n",
       "  0.1303473002748283,\n",
       "  0.1303430721386246,\n",
       "  0.13033811556465077,\n",
       "  0.1303326552650962,\n",
       "  0.13032681580572292,\n",
       "  0.13032072240701909,\n",
       "  0.13031440230889926,\n",
       "  0.1303079746664527,\n",
       "  0.13030141192140512,\n",
       "  0.13029482702371428,\n",
       "  0.13028820965910493,\n",
       "  0.13028161176124964,\n",
       "  0.13027500807746048,\n",
       "  0.13026843293007306,\n",
       "  0.13026185806215065,\n",
       "  0.13025482998410723,\n",
       "  0.13024731639756756,\n",
       "  0.13023946897351782,\n",
       "  0.130231443691603,\n",
       "  0.13022328707986464,\n",
       "  0.1302151594496553,\n",
       "  0.130207065360288,\n",
       "  0.1301990635224616,\n",
       "  0.1301912062321449,\n",
       "  0.1301834538002658,\n",
       "  0.13017531482075229,\n",
       "  0.13016679738800555,\n",
       "  0.13015805091928498,\n",
       "  0.13014920277500264,\n",
       "  0.130140397337006,\n",
       "  0.13013164043516548,\n",
       "  0.13012307849041463,\n",
       "  0.13011465209339199,\n",
       "  0.13010633040210964,\n",
       "  0.1300982522609744,\n",
       "  0.13009033489432573,\n",
       "  0.1300825778001349,\n",
       "  0.1300751124299512,\n",
       "  0.130067723604367,\n",
       "  0.13006047491175382,\n",
       "  0.13005339564407734,\n",
       "  0.13004649716360658,\n",
       "  0.13003966309693746,\n",
       "  0.13003298995745227,\n",
       "  0.1300263591360269,\n",
       "  0.13001988366545023,\n",
       "  0.13001347045578637,\n",
       "  0.13000714607421718,\n",
       "  0.13000085969196518,\n",
       "  0.12999465559631004,\n",
       "  0.12998844396806475,\n",
       "  0.1299823645469326,\n",
       "  0.12997583750497943,\n",
       "  0.12996902237998714,\n",
       "  0.1299615083497608,\n",
       "  0.12995346813087383,\n",
       "  0.12994507185196724,\n",
       "  0.12993641556833116,\n",
       "  0.12992774531936804,\n",
       "  0.12991908241932607,\n",
       "  0.12991047392420518,\n",
       "  0.12990198764532365,\n",
       "  0.129893696393398,\n",
       "  0.12988546397893894,\n",
       "  0.12987751540656553,\n",
       "  0.12986971568747963,\n",
       "  0.12986217009931625,\n",
       "  0.12985478926801566,\n",
       "  0.12984744090363365,\n",
       "  0.1298403783375923,\n",
       "  0.12983334971862565,\n",
       "  0.12982653006097036,\n",
       "  0.12981980152403108,\n",
       "  0.12981319542589478,\n",
       "  0.12980671238239283,\n",
       "  0.12980030722391397,\n",
       "  0.12979397610781448,\n",
       "  0.12978773661445966,\n",
       "  0.1297815956476434,\n",
       "  0.12977549825647952,\n",
       "  0.12976942283428983,\n",
       "  0.12976346455403295,\n",
       "  0.12975753745213048,\n",
       "  0.12975165588213533,\n",
       "  0.1297457841503166,\n",
       "  0.12973995259663368,\n",
       "  0.12973409921370904,\n",
       "  0.12972834875889674,\n",
       "  0.1297225966402593,\n",
       "  0.1297168786343588,\n",
       "  0.1297111288342725,\n",
       "  0.1297054021210204,\n",
       "  0.12969978175932698,\n",
       "  0.1296940953769081,\n",
       "  0.12968846436457296,\n",
       "  0.12968247497870902,\n",
       "  0.1296761712609837,\n",
       "  0.12966955580149012,\n",
       "  0.129662794112655,\n",
       "  0.1296559743808118,\n",
       "  0.12964910461464965,\n",
       "  0.1296422189708419,\n",
       "  0.129635438351134,\n",
       "  0.1296287237651164,\n",
       "  0.12962198575589823,\n",
       "  0.12961511240855053,\n",
       "  0.12960807645218705,\n",
       "  0.12960091000439505,\n",
       "  0.12959369422094658,\n",
       "  0.12958617772667444,\n",
       "  0.12957827104163908,\n",
       "  0.12957023042372215,\n",
       "  0.12956213956188747,\n",
       "  0.1295539220383032,\n",
       "  0.12954592557585648,\n",
       "  0.12953797024374333,\n",
       "  0.12953015631515855,\n",
       "  0.12952244306600127,\n",
       "  0.12951493855559174,\n",
       "  0.12950756129179156,\n",
       "  0.12950042579654913,\n",
       "  0.1294934171856199,\n",
       "  0.12948651813613535,\n",
       "  0.12947977330667465,\n",
       "  0.12947315878497342,\n",
       "  0.12946667809927734,\n",
       "  0.1294603018877545,\n",
       "  0.12945408683498255,\n",
       "  0.12944793474178587,\n",
       "  0.12944188724052672,\n",
       "  0.12943588742406378,\n",
       "  0.12942999938279723,\n",
       "  0.129424152960928,\n",
       "  0.1294184505975047],\n",
       " 'test_losses': [0.020637617844801684,\n",
       "  0.020516978043776293,\n",
       "  0.02040435497577374,\n",
       "  0.020296466533954328,\n",
       "  0.020191169151893028,\n",
       "  0.02008784367487981,\n",
       "  0.019986220139723558,\n",
       "  0.019886402717003454,\n",
       "  0.019788331251878004,\n",
       "  0.019691998408390924,\n",
       "  0.019597317622258113,\n",
       "  0.019504893376277043,\n",
       "  0.019414662581223707,\n",
       "  0.01932669419508714,\n",
       "  0.01924056419959435,\n",
       "  0.01915714557354267,\n",
       "  0.019076191828801083,\n",
       "  0.018998219416691706,\n",
       "  0.018922507946307843,\n",
       "  0.018849496107835036,\n",
       "  0.01877863517174354,\n",
       "  0.01871099178607647,\n",
       "  0.018646143399752103,\n",
       "  0.018583787771371693,\n",
       "  0.01852386474609375,\n",
       "  0.018467493790846605,\n",
       "  0.0184139031630296,\n",
       "  0.018362640967735877,\n",
       "  0.018313884735107422,\n",
       "  0.018267990992619443,\n",
       "  0.0182252443753756,\n",
       "  0.01818511522733248,\n",
       "  0.01814752432016226,\n",
       "  0.018112542079045224,\n",
       "  0.018080567580003006,\n",
       "  0.01805118120633639,\n",
       "  0.018024221567007212,\n",
       "  0.01799965344942533,\n",
       "  0.017977545811579777,\n",
       "  0.017957637493426982,\n",
       "  0.017939775907076322,\n",
       "  0.017923912635216345,\n",
       "  0.017909966982327975,\n",
       "  0.017897899334247295,\n",
       "  0.017887382507324218,\n",
       "  0.017878054105318508,\n",
       "  0.017869739532470702,\n",
       "  0.017862240717961237,\n",
       "  0.017855325845571663,\n",
       "  0.01784905213576097,\n",
       "  0.01784320537860577,\n",
       "  0.017837616847111628,\n",
       "  0.017832051790677584,\n",
       "  0.017826345883882964,\n",
       "  0.017820409628061147,\n",
       "  0.01781415792611929,\n",
       "  0.01780752622164213,\n",
       "  0.01780049837552584,\n",
       "  0.01779299809382512,\n",
       "  0.017785067925086388,\n",
       "  0.01777681203988882,\n",
       "  0.01776812039888822,\n",
       "  0.017759085435133715,\n",
       "  0.017749748229980468,\n",
       "  0.017740176274226263,\n",
       "  0.01773040331326998,\n",
       "  0.017720482165996844,\n",
       "  0.017710506732647235,\n",
       "  0.017700421259953424,\n",
       "  0.01769030057466947,\n",
       "  0.01768015347994291,\n",
       "  0.017670023991511418,\n",
       "  0.017659885699932393,\n",
       "  0.017649850111741285,\n",
       "  0.017639846801757814,\n",
       "  0.01762992712167593,\n",
       "  0.017620016244741587,\n",
       "  0.017610143514779898,\n",
       "  0.017600352947528545,\n",
       "  0.017590543306790864,\n",
       "  0.017580777681790866,\n",
       "  0.01757101499117338,\n",
       "  0.017561202416053186,\n",
       "  0.017551413315993088,\n",
       "  0.017541638887845552,\n",
       "  0.017531915811391977,\n",
       "  0.0175222294147198,\n",
       "  0.01751261931199294,\n",
       "  0.017502987201397237,\n",
       "  0.017493457794189454,\n",
       "  0.01748398120586689,\n",
       "  0.01747447087214543,\n",
       "  0.017464978144719052,\n",
       "  0.017455519162691557,\n",
       "  0.0174461423433744,\n",
       "  0.0174368403508113,\n",
       "  0.017427611717810997,\n",
       "  0.01741840069110577,\n",
       "  0.017409251286433294,\n",
       "  0.017400164970984826,\n",
       "  0.0173911138681265,\n",
       "  0.01738221682035006,\n",
       "  0.017373356452355018,\n",
       "  0.01736454890324519,\n",
       "  0.017355854327862078,\n",
       "  0.01734721697293795,\n",
       "  0.01733859869150015,\n",
       "  0.01733003469613882,\n",
       "  0.017321489774263822,\n",
       "  0.017312991802509014,\n",
       "  0.01730452904334435,\n",
       "  0.017296092693622294,\n",
       "  0.017287738506610577,\n",
       "  0.017279363778921274,\n",
       "  0.017271036001352165,\n",
       "  0.017262715559739333,\n",
       "  0.017254439133864184,\n",
       "  0.017246183248666617,\n",
       "  0.01723790975717398,\n",
       "  0.017229715494009163,\n",
       "  0.01722147428072416,\n",
       "  0.017213213993952824,\n",
       "  0.017204958108755258,\n",
       "  0.017196734501765325,\n",
       "  0.017188538771409256,\n",
       "  0.01718036358173077,\n",
       "  0.01717222653902494,\n",
       "  0.017164117372952974,\n",
       "  0.017156075697678786,\n",
       "  0.01714807363656851,\n",
       "  0.01714011412400466,\n",
       "  0.017132225036621093,\n",
       "  0.017124366760253907,\n",
       "  0.017116531958946815,\n",
       "  0.01710873970618615,\n",
       "  0.01710096212533804,\n",
       "  0.01709323442899264,\n",
       "  0.017085530207707332,\n",
       "  0.017077852395864634,\n",
       "  0.017070192190317007,\n",
       "  0.017062552525446965,\n",
       "  0.017054951007549578,\n",
       "  0.017047353891225962,\n",
       "  0.017039794921875,\n",
       "  0.01703225649320162,\n",
       "  0.017024741539588342,\n",
       "  0.017017235389122597,\n",
       "  0.017009749779334435,\n",
       "  0.01700229791494516,\n",
       "  0.01699484605055589,\n",
       "  0.016987424997182993,\n",
       "  0.016980018615722656,\n",
       "  0.01697262984055739,\n",
       "  0.016965227860670822,\n",
       "  0.016957872830904448,\n",
       "  0.016950533940241888,\n",
       "  0.01694320825430063,\n",
       "  0.01693589137150691,\n",
       "  0.01692858182466947,\n",
       "  0.016921310424804686,\n",
       "  0.016914036090557392,\n",
       "  0.016906766157883866,\n",
       "  0.016899522634652945,\n",
       "  0.016892261505126953,\n",
       "  0.016885025317852315,\n",
       "  0.016877833146315354,\n",
       "  0.016870665917029747,\n",
       "  0.01686352362999549,\n",
       "  0.01685639161330003,\n",
       "  0.01684930361234225,\n",
       "  0.016842202406663162,\n",
       "  0.016835130544809195,\n",
       "  0.016828046945425182,\n",
       "  0.016820994157057543,\n",
       "  0.016813958974984975,\n",
       "  0.016806914989764873,\n",
       "  0.016799891545222354,\n",
       "  0.01679292972271259,\n",
       "  0.016786022186279295,\n",
       "  0.016779173337496244,\n",
       "  0.016772362635685847,\n",
       "  0.016765571007361778,\n",
       "  0.016758802854097806,\n",
       "  0.016752061110276443,\n",
       "  0.01674533403836764,\n",
       "  0.016738639244666467,\n",
       "  0.016731978196364183,\n",
       "  0.016725315680870642,\n",
       "  0.016718694246732273,\n",
       "  0.01671209041888897,\n",
       "  0.016705505664532,\n",
       "  0.01669892824613131,\n",
       "  0.016692378704364484,\n",
       "  0.016685854104849008,\n",
       "  0.01667934124286358,\n",
       "  0.01667284451998197,\n",
       "  0.016666350731482874,\n",
       "  0.016659877483661358,\n",
       "  0.016653436514047475,\n",
       "  0.01664697500375601],\n",
       " 'bin_size': 20}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[cell_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46ad10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
